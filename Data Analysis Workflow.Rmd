---
title: "Data Analysis Workflow"
---
# 0. Step 0 Get ready
## 0.1 Set working directory
```{r}
getwd()
setwd()
```
**Relative** - `setwd("./data")`, `setwd("../")` (Move up one directory)  
**Absolute** - `setwd("/Users/jtleek/data/")` "/" for path in windows
## 0.2 Check for and create directories
```{r}
# check to see if the directory exists
file.exists("directoryName") 
# create a directory if it doesn't exist
dir.create("directoryName") 

# an example checking for a "data" directory and creating it if it doesn't exist
if(!file.exists("data")){
  dir.create("data")
}
```
# Packages and Tips
## {base}
rewrite the names of the columns to remove any spaces
```{r}
names(ozone) <- make.names(names(ozone))
```
## {ggplot2}
I've coloured the models by `-dist`: this is an easy way to make sure that the best models (i.e. the ones with the smallest distance) get the brighest colours.
```{r}
ggplot(sim1, aes(x, y)) + 
  geom_point(size = 2, colour = "grey30") + 
  geom_abline(
    aes(intercept = a1, slope = a2, colour = -dist), 
    data = filter(models, rank(dist) <= 10)
  )
```
## {data.table}
Much, much faster at subsetting, group, and updating
### Create data tables just like data frames
```{r}
library(data.table)
DF = data.frame(x=rnorm(9),y=rep(c("a","b","c"),each=3),z=rnorm(9))
head(DF,3)
DT = data.table(x=rnorm(9),y=rep(c("a","b","c"),each=3),z=rnorm(9))
head(DT,3)
```
### See all the data tables in memory
```{r}
tables()
```
### Subset rows
```{r}
DT[2,]
DT[DT$y=="a",]
```
### Subsetting is based on rows (not columns like DF[c(2,3)])
```{r}
DT[c(2,3)]
```
**Data table use expression after comma**
### Calculate values for variables with expressions
```{r}
DT[,list(mean(x),sum(z))]
DT[,table(y)]
```
### Add new columns
```{r}
DT[,w:=z^2]
```
**When add a new variable to a data table, a new copy isn't being created.  If you're trying to create a copy, you have to explicitly do that with the copy function.** 
### Multiple operations
```{r}
DT[,m:= {tmp <- (x+z); log2(tmp+5)}]
```
### plyr like operations
```{r}
DT[,a:=x>0]
```

```{r}
DT[,b:= mean(x+w),by=a]
```
`by=a` aggregated/grouped by a. Then add aggregated values (mean in this case) of different groups to each row of b.
### Special variables
`.N` An integer, length 1, containing the number of elements (counts) of a factor level
```{r}
set.seed(123);
DT <- data.table(x=sample(letters[1:3], 1E5, TRUE))
DT[, .N, by=x]
```
### Keys
#### To subset
```{r}
DT <- data.table(x=rep(c("a","b","c"),each=100), y=rnorm(300))
setkey(DT, x)
DT['a']
```
#### To Join(merge)
```{r}
DT1 <- data.table(x=c('a', 'a', 'b', 'dt1'), y=1:4)
DT2 <- data.table(x=c('a', 'b', 'dt2'), z=5:7)
setkey(DT1, x); setkey(DT2, x)
merge(DT1, DT2)
```
### Fast reading from disk
```{r}
big_df <- data.frame(x=rnorm(1E6), y=rnorm(1E6))
file <- tempfile()
write.table(big_df, file=file, row.names=FALSE, col.names=TRUE, sep="\t", quote=FALSE)
system.time(fread(file))
system.time(read.table(file, header=TRUE, sep="\t"))
```
`fread()` could be applied to reading data tables, a substitute for `read.table()` with tab separated files
### Further resources
- [Latest development of data.table](https://github.com/Rdatatable/data.table/wiki)
- [A list of differences between data.table and data.frame](https://stackoverflow.com/questions/13618488/what-you-can-do-with-data-frame-that-you-cant-in-data-table)
## {sqldf}
sqldf package allows for execution of SQL commands on R data frames

# 1. Step 1 Get the data
## 1.1 From the internet
```{r}
# Download the file
fileUrl <- "https://data.baltimorecity.gov/api/views/dz54-2aru/rows.csv?accessType=DOWNLOAD"
download.file(fileUrl,destfile="./data/cameras.csv",mode = 'wb')

# Check the downloaded file
list.files("./data")
# Record the date downloaded
dateDownloaded <- date()
dateDownloaded
```
## 1.2 From local flat files
### 1.2.1 {base}
#### read.table()
```{r}
cameraData <- read.table("./data/cameras.csv",sep=",",header=TRUE)
```
Important parameters
- `file`
- `header`
- `sep` - delimiter
- `row.names`
- `nrows` - how many rows to read of the file (e.g. nrows=10 reads 10 lines)
- `quote` - whether there are any quoted values, quote="" means no quotes
- `na.strings` - set the character that represents a missing value
- `skip` - number of lines to skip before starting to read
- `colClasses`
#### read.csv()
```{r}
cameraData <- read.csv("./data/cameras.csv")
```
`read.csv` sets `sep=","` and `header=TRUE`
### 1.2.2 {readr}
#### read_csv()
```{r}
library(readr)
ozone <- read_csv("data/hourly_44201_2014.csv", col_types = "ccccinnccccccncnncccccc")
```
`col_types=` c = character, i = integer, n = number, d = double, l = logical, D = date, T = date time, t = time, ? = guess, or _/- to skip the column
## 1.3 From Excel files
### 1.3.1 {readxl}
#### read_excel()
```{r}
download.file(fileUrl,destfile="./data/cameras.xlsx",mode="wb")
library(readxl)
cameraData <- read_excel("./data/cameras.xlsx",sheet=1,col_names = TRUE,range=cell_limits(ul=c(18,7),lr=c(23,15)))
```
## 1.3 From XML or Html
### 1.3.1 {XML}
```{r}
library(XML)
fileUrl <- "http://www.w3schools.com/xml/simple.xml"
doc <- xmlTreeParse(fileUrl,useInternal=TRUE) # or htmlTreeParse() if from Html
```
`useInternal=TRUE` get all the different nodes inside of the file
1. Inspect the data 
```{r}
# get rootNode, the wrapper for the entire document
rootNode <- xmlRoot(doc)
# get the name of the document
xmlName(rootNode)
# get all the nested elements within the rootNode
names(rootNode)
```

```{r}
# Directly access parts of the XML document
rootNode[[1]]
# access subcomponent
rootNode[[1]][[1]]
```
2. Programatically extract parts of the file
```{r}
# get all values
xmlSApply(rootNode,xmlValue)
```
To be more specific, use [XPath](http://www.stat.berkeley.edu/~statcur/Workshop2/Presentations/XML.pdf)
- `/node` Top level node
- `//node` Node at any level
- `node[@attr-name]` Node with an attribute name
- `node[@attr-name='bob']` Node with attribute name attr-name='bob'
```{r}
# examples
xpathSApply(rootNode,"//name",xmlValue)
xpathSApply(rootNode,"//price",xmlValue)
```
2.1 Extract content by attributes
```{r}
# example
fileUrl <- "http://espn.go.com/nfl/team/_/name/bal/baltimore-ravens"
doc <- htmlTreeParse(fileUrl,useInternal=TRUE)
scores <- xpathSApply(doc,"//li[@class='score']",xmlValue)
teams <- xpathSApply(doc,"//li[@class='team-name']",xmlValue)
```
### 1.3.2 Notes and further resources
- Official XML tutorials [short](http://www.omegahat.org/RSXML/shortIntro.pdf), [long](http://www.omegahat.org/RSXML/shortIntro.pdf)
- [An outstanding guide to the XML package](https://www.stat.berkeley.edu/~statcur/Workshop2/Presentations/XML.pdf)
## 1.4 From JSON
### 1.4.1 {jsonlite}
```{r}
library(jsonlite)
jsonData <- fromJSON("https://api.github.com/users/jtleek/repos")
```
1. Inspect the data (nested objects)
```{r}
names(jsonData)
jsonData$name

names(jsonData$owner)
jsonData$owner$login
```
2. Writing data frames to JSON
```{r}
myjson <- toJSON(iris, pretty=TRUE)
# print the data
cat(myjson)
```
`pretty=TRUE` give nice indentations so it's easy to read
### 1.4.2 Further resources
- [http://www.json.org/](http://www.json.org/)
- [A good tutorial on jsonlite](https://www.r-bloggers.com/new-package-jsonlite-a-smarter-json-encoderdecoder/)
- [jsonlite vignette](https://cran.r-project.org/web/packages/jsonlite/vignettes/json-mapping.pdf)
## 1.5 From MySQL
### 1.5.1 {RMySQL}
1. Connect and list databases
```{r}
library(RMySQL)
ucscDb <- dbConnect(MySQL(),user="genome", 
                    host="genome-mysql.cse.ucsc.edu")
result <- dbGetQuery(ucscDb,"show databases;"); dbDisconnect(ucscDb)
```
2. Connect to specific database and list tables
```{r}
hg19 <- dbConnect(MySQL(),user="genome", db="hg19",
                    host="genome-mysql.cse.ucsc.edu")
allTables <- dbListTables(hg19)
length(allTables)
allTables[1:5]
```
3. Get dimensions of a specific table
```{r}
# get column names
dbListFields(hg19,"affyU133Plus2")
# get number of rows
dbGetQuery(hg19, "select count(*) from affyU133Plus2")
```
4. Read from the table
```{r}
affyData <- dbReadTable(hg19, "affyU133Plus2")
head(affyData)
```
4.1 Select a specific subset
```{r}
query <- dbSendQuery(hg19, "select * from affyU133Plus2 where misMatches between 1 and 3")
affyMis <- fetch(query)
affyMisSmall <- fetch(query,n=10); dbClearResult(query)
dim(affyMisSmall)
```
5. Close the connection
```{r}
dbDisconnect(hg19)
```
### 1.5.2 Further resources
- [RMySQL vignette](https://cran.r-project.org/web/packages/RMySQL/RMySQL.pdf)
- [List of MySQL commands](http://www.pantz.org/software/mysql/mysqlcommands.html)
- A nice blog post summarizing [some other commands](https://www.r-bloggers.com/mysql-and-r/)
## 1.6 From HDF5
### 1.6.1 {rhdf5}
```{r}
source("http://bioconductor.org/biocLite.R")
biocLite("rhdf5")
library(rhdf5)
```
1. Create HDF5 file
```{r}
created = h5createFile("example.h5")
```
1.1 Create groups
```{r}
created = h5createGroup("example.h5","foo")
created = h5createGroup("example.h5","baa")
created = h5createGroup("example.h5","foo/foobaa")
h5ls("example.h5")
```
1.2 Write to groups
```{r}
A = matrix(1:10,nr=5,nc=2)
h5write(A, "example.h5","foo/A")
B = array(seq(0.1,2.0,by=0.1),dim=c(5,2,2))
attr(B, "scale") <- "liter"
h5write(B, "example.h5","foo/foobaa/B")
h5ls("example.h5")
```
1.3 Write a data set
```{r}
df = data.frame(1L:5L,seq(0,1,length.out=5),
  c("ab","cde","fghi","a","s"), stringsAsFactors=FALSE)
h5write(df, "example.h5","df")
h5ls("example.h5")
```
2. Read data
```{r}
readA = h5read("example.h5","foo/A")
readB = h5read("example.h5","foo/foobaa/B")
readdf= h5read("example.h5","df")
readA
```
3. Write and read chunks(subsets)
```{r}
h5write(c(12,13,14),"example.h5","foo/A",index=list(1:3,1))
h5read("example.h5","foo/A")
```
`index=list(1:3,1)` give you indices for the dimensions to read/write
#### 1.6.2 Further resources
- [The rhdf5 tutorial](http://www.bioconductor.org/packages/release/bioc/html/rhdf5.html)
- The HDF group has [informaton on HDF5 in general](https://portal.hdfgroup.org/display/HDF5/HDF5)
## 1.7 From Web
### 1.7.1 {base}
#### readLines()
```{r}
# open a connection
con = url("http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en")
htmlCode = readLines(con)
# close the connection
close(con)
```
### 1.7.2 {XML}
```{r}
library(XML)
url <- "http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en"
html <- htmlTreeParse(url, useInternalNodes=T)

xpathSApply(html, "//title", xmlValue)
xpathSApply(html, "//td[@id='col-citedby']", xmlValue)
```
`useInternalNodes=T` gets the complete structure out
### 1.7.3 {httr}
#### GET()
```{r}
library(httr)
# GET the URL
html2 = GET(url)
# extract the content from that HTML page as a text
content2 = content(html2,as="text")
# parse the content
parsedHtml = htmlParse(content2,asText=TRUE)
xpathSApply(parsedHtml, "//title", xmlValue)
```
1. Access websites with passwords
```{r}
pg2 = GET("http://httpbin.org/basic-auth/user/passwd",
    authenticate("user","passwd"))
```
1.1 Use handles (to store authentication)
```{r}
google = handle("http://google.com")
pg1 = GET(handle=google,path="/")
pg2 = GET(handle=google,path="search")
```
### 1.7.4 Further resources
- [R Bloggers](https://www.r-bloggers.com/) has a number of examples of web scraping
- The [httr help file](https://cran.r-project.org/web/packages/httr/httr.pdf) has useful examples
## 1.8 From APIs
### 1.8.1 {httr}
1. Access Website from R
```{r}
# example 1: Twitter
library(httr)
myapp = oauth_app("twitter",
                   key="yourConsumerKeyHere",secret="yourConsumerSecretHere")
sig = sign_oauth1.0(myapp,
                     token = "yourTokenHere",
                      token_secret = "yourTokenSecretHere")
homeTL = GET("https://api.twitter.com/1.1/statuses/home_timeline.json", sig)
```

```{r}
# example 2: GitHub
# 1. Find OAuth settings for github:
#    http://developer.github.com/v3/oauth/
oauth_endpoints("github")
# 2. To make your own application, register at 
#    https://github.com/settings/developers. Use any URL for the homepage URL
#    (http://github.com is fine) and  http://localhost:1410 as the callback url
#    Replace your key and secret below.
myapp <- oauth_app("datasciencespecialization",
                   key = "key",
                   secret = "secret")
# 3. Get OAuth credentials
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
# 4. Use API
gtoken <- config(token = github_token)
req <- GET("https://api.github.com/users/jtleek/repos", gtoken)
stop_for_status(req)
content(req)
# OR:
req <- with_config(gtoken, GET("https://api.github.com/users/jtleek/repos"))
stop_for_status(req)
content(req)
```
2. Convert the json object
```{r}
# example 1
# return a structured R object, which is a little bit hard to read
json1 = content(homeTL)
```
### 1.8.2 {jsonlite}
```{r}
# example 1
# reformat it as a data frame where each row corresponds to a tweet in the user's timeline
json2 = jsonlite::fromJSON(toJSON(json1))
```

```{r}
# example 2
library(jsonlite)
repo <- fromJSON(toJSON(content(req)))
```
## 1.9 From image
- [jpeg](https://cran.r-project.org/web/packages/jpeg/index.html)
- [readbitmap](https://cran.r-project.org/web/packages/readbitmap/index.html)
- [png](https://cran.r-project.org/web/packages/png/index.html)
- [EBImage (Bioconductor)](http://www.bioconductor.org/packages/2.13/bioc/html/EBImage.html)
## 1.10 From GIS data
- [rgdal](https://cran.r-project.org/web/packages/rgdal/index.html)
- [rgeos](https://cran.r-project.org/web/packages/rgeos/index.html)
- [raster](https://cran.r-project.org/web/packages/raster/index.html)
## 1.11 From music data
- [tuneR](https://cran.r-project.org/web/packages/tuneR/)
- [seewave](http://rug.mnhn.fr/seewave/)

# 2. Step 2 Clean the data
## 2.1 Data transformation
### 2.1.1 Subset
#### 2.1.1.1 {base}
1. Position or name
```{r}
# subset by colunmns, x is a data frame
X[,1]
# or
X[1]
# or
X[,'var1']

# subset by rows and colunmns
X[1:2,'var2']
```
2. Logicals (and, or, %in%)
```{r}
X[(X$var1 <= 3 & X$var3 > 11),]
X[(X$var1 <= 3 | X$var3 > 15),]
restData[restData$zipCode %in% c("21212","21213"),]
```
3. Deal with missing values
```{r}
X[which(X$var2 > 8),]
```
`which()` returns the indices where conditions are met
#### 2.1.1.2 {dplyr} or {tidyverse}
##### filter() (rows)
```{r}
donor %>% filter(amount > 100)
donor %>% filter(! type %in% 'Candidate')
```
Operators for conditions: `!, %in%, >, >=, <, <=, ==, &, |`
##### select() (columns)
```{r}
select(chicago, 1:5)
select(chicago, city:dptp)
select(chicago, -(city:dptp))
```
### 2.1.2 Reorder
#### 2.1.2.1 {dplyr} or {tidyverse}
##### arrange()
```{r}
arrange(chicago, desc(date))
```
#### 2.1.2.2 {base}
##### sort()
```{r}
sort(X$var1)
sort(X$var1,decreasing=TRUE)
sort(X$var2,na.last=TRUE)
```
##### order()
```{r}
X[order(X$var1),]
X[order(X$var1,X$var3),]
```
`order()` returns a permutation which rearranges its first argument into ascending or descending order, breaking ties by further arguments.
### 2.1.3 Add rows and columns
#### 2.1.3.1 {base}
```{r}
X$var4 <- rnorm(5)
```
##### cbind() and rbind()
```{r}
Y <- cbind(X,rnorm(5))
```
#### 2.1.3.2 {dplyr} or {tidyverse}
##### bind_rows() and bind_cols()
Append z to y as new rows. 
```{r}
bind_rows(y, z) 
```
Append z to y as new columns.
```{r}
bind_cols(y, z) 
```
Caution: matches rows by position.
### 2.1.4 Rename 
#### 2.1.4.1 {dplyr} or {tidyverse}
##### rename()
```{r}
rename(chicago, dewpoint = dptp, pm25 = pm25tmean2)
```
### 2.1.5 Create new variables
#### 2.1.5.1 {dplyr} or {tidyverse}
##### mutate()
```{r}
library(tidyverse)
restData2 = mutate(restData,zipGroups=cut2(zipCode,g=4))
```
#### 2.1.5.2 {base} 
- Create sequences
```{r}
s1 <- seq(1,10,by=2)
s2 <- seq(1,10,length=3)
x <- c(1,3,8,25,100); seq(along = x)
```
- Create binary variables
```{r}
restData$nearMe = restData$neighborhood %in% c("Roland Park", "Homeland")
restData$zipWrong = ifelse(restData$zipCode < 0, TRUE, FALSE)
```
#### Common transforms
- `abs(x)` absolute value
- `sqrt(x)` square root
- `ceiling(x)` ceiling(3.475) is 4
- `floor(x)` floor(3.475) is 3
- `round(x,digits=n)` round(3.475,digits=2) is 3.48
- `signif(x,digits=n)` signif(3.475,digits=2) is 3.5
- `cos(x)`, `sin(x)` etc.
- `log(x)` natural logarithm
- `log2(x)`, `log10(x)` other common logs
- `exp(x)` exponentiating x
- [http://statmethods.net/management/functions.html](http://statmethods.net/management/functions.html)
### 2.1.6 Reshape the data
#### 2.1.6.1 {tidyr} or {tidyverse}
##### Convert columns into rows (with `gather()`)
```{r}
donor %>% gather(column_name, year, c(receipt_year, election_year))
```
`key=column_name` (new) name of column that store (old) column names  
`value=year` (new) name of column that store (old) values in (old) columns  
`c(receipt_year, election_year)` A selection of columns. If empty, all variables are selected. You can supply bare variable names, select all variables between x and z with `x:z`, exclude y with `-y`.
##### Convert rows into columns (with `spread()`)
```{r}
donor %>% spread(receipt_year, amount)
```
`key=receipt_year` (old) name or position of column that contains the names of the (new) columns  
`value=amount` (old) name of column that contains the values of the (new) columns
#### 2.1.6.2 {reshape2}
```{r}
library(reshape2)
```
##### melt() data frames
```{r}
mtcars$carname <- rownames(mtcars)
carMelt <- melt(mtcars,id=c("carname","gear","cyl"),measure.vars=c("mpg","hp"))
```
`id=` creates an id value for car name, for gear and for cylinders. And then melt all the rest of the values. 
##### Cast (dcast()) data frames
```{r}
cylData <- dcast(carMelt, cyl ~ variable)
cylData <- dcast(carMelt, cyl ~ variable,mean)
```
Default aggregation function is length()
#### 2.1.6.3 Further resources
- [A nice reshape tutorial](https://www.slideshare.net/jeffreybreen/reshaping-data-in-r)
### 2.1.7 Aggregate values / Create a new variable
#### 2.1.7.1 {dplyr} or {tidyverse}
##### group_by(), summarise()
###### Example 1
In `donor`, which `type` received the more money in donations from `party` 'REPUBLICAN'?
```{r}
donor %>% group_by(type, party) %>% summarise(dollars = sum(amount, na.rm = TRUE))
```
- `group_by()` lists the variables by which to aggregate data
- `summarise()` creates a variable (dollars) and define the variable with an aggregation function
- Use `%>%` to 'link' `group_by()` and `summarise()`
- Aggregation functions: `n(), n_distinct(), sum(), mean(), max(), min()`, etc.
- `options(pillar.sigfig = 10)` to see more digits for numeric results in tibble
###### Example 2
In `donor`, what is the largest average donation `amount` for `contributor_state`?
```{r}
donor %>% 
  group_by(contributor_state) %>% 
  summarise(avg_donation = mean(amount, na.rm = TRUE)) %>%
  arrange(desc(avg_donation))
```
###### Example 3
```{r}
police %>% 
  transmute(
    initial_type_group = initial_type_group %>% as.character()
    , district_sector = district_sector %>% as.character()
    , event_clearance_group = event_clearance_group %>% as.character()
  ) %>%
  mutate(
    initial_type_group = ifelse(initial_type_group %in% NA, 'Unknown', initial_type_group)
  ) %>%
  filter(event_clearance_group %in% 'DISTURBANCES') %>% 
  group_by(initial_type_group, district_sector) %>% 
  summarise(n  = n()) %>%
  write_csv('tidyverse_exercise_output.csv')
```
###### Example 4
```{r}
police %>% 
  transmute(
    initial_type_group = initial_type_group %>% as.character()
    , district_sector = district_sector %>% as.character()
    , event_clearance_group = event_clearance_group %>% as.character()
  ) %>%
  mutate(
    initial_type_group = ifelse(initial_type_group %in% NA, 'Unknown', initial_type_group)
  ) %>%
  filter(event_clearance_group %in% 'DISTURBANCES') %>% 
  group_by(initial_type_group, district_sector) %>% 
  summarise(n  = n()) %>%
  filter(
    (initial_type_group %in% 'Unknown' & district_sector %in% 'W') |
    (initial_type_group %in% 'ROAD RAGE' & district_sector %in% 'L')
  )
```
#### 2.1.7.2 {base}
##### tapply()
```{r}
tapply(InsectSprays$count,InsectSprays$spray,sum)
```
sum up the counts, break down by spray
##### Another way - split()+apply()+unlist()
```{r}
spIns =  split(InsectSprays$count,InsectSprays$spray)
```
`split()` returns a list
```{r}
sprCount = lapply(spIns,sum)
unlist(sprCount)
```
##### Another way - split()+sapply()
```{r}
sapply(spIns,sum)
```
#### 2.1.7.3 {plyr}
##### ddply()
```{r}
library(plyr)
ddply(InsectSprays,.(spray),summarize,sum=sum(count))
```
`.()` are the variables to summarize
```{r}
spraySums <- ddply(InsectSprays,.(spray),summarize,sum=ave(count,FUN=sum))
```
##### Further resources
- A [tutorial](http://plyr.had.co.nz/09-user/) from the developer of plyr
- [A good plyr primer](https://www.r-bloggers.com/a-quick-primer-on-split-apply-combine-problems/)
### 2.1.8 Merge the data
#### 2.1.8.1 {dplyr} or {tidyverse}
##### Mutating Joins
Join matching rows from b to a.
```{r}
left_join(a, b, by = "x1")
```
Join matching rows from a to b. 
```{r}
right_join(a, b, by = "x1") 
```
Join data. Retain only rows in both sets. 
```{r}
inner_join(a, b, by = "x1") 
```
Join data. Retain all values, all rows.
```{r}
full_join(a, b, by = "x1") 
```
##### Filtering Joins
All rows in a that have a match in b. 
```{r}
semi_join(a, b, by = "x1") 
```
All rows in a that do not have a match in b
```{r}
anti_join(a, b, by = "x1") 
```
##### Set Operations
Rows that appear in both y and z. 
```{r}
intersect(y, z) 
```
Rows that appear in either or both y and z.
```{r}
union(y, z)
```
Rows that appear in y but not z.
```{r}
setdiff(y, z)
```
#### 2.1.8.2 {base}
##### merge()
```{r}
mergedData = merge(reviews,solutions,by.x="solution_id",by.y="id",all=TRUE)
```
`all=TRUE` full join
#### 2.1.8.3 Further resourece
- [The quick R data merging page](https://www.statmethods.net/management/merging.html)
## 2.2 Data types
### 2.2.1 Strings
#### 2.2.1.1 Transformation of upper/lower case
##### {base}
```{r}
tolower(names(cameraData))
toupper()
```
##### {stringr}
```{r}
str_to_lower()
str_to_upper()
str_to_upper(c("i", "ı"), locale = "tr")
str_to_title()
```
pick the set of rules to use by specifying a `locale`
#### 2.2.1.2 Sort and order
##### {stringr}
```{r}
x <- c("apple", "eggplant", "banana")

str_sort(x, locale = "en")  # English
#> [1] "apple"    "banana"   "eggplant"

str_sort(x, locale = "haw") # Hawaiian
#> [1] "apple"    "eggplant" "banana"
```
#### 2.2.1.3 Split
##### {base}
###### strsplit()+sapply()
```{r}
splitNames = strsplit(names(cameraData),"\\.")
```
`split="\\."` character vector containing regular expression(s) to use for splitting.
- strsplit() returns a list
- use escape character '\' because the period is a reserved character
Then extract the first element of splited names:
```{r}
splitNames[[6]][1]
firstElement <- function(x){x[1]}
sapply(splitNames,firstElement)
```
##### {stringr}
###### str_split()
```{r}
# split sentences into words
sentences %>%
  head(5) %>% 
  str_split(" ")
```
Because each component might contain a different number of pieces, this returns a list. If you're working with a length-1 vector, the easiest thing is to just extract the first element of the list:
```{r}
"a|b|c|d" %>% 
  str_split("\\|") %>% 
  .[[1]]
```
use `simplify = TRUE` to return a matrix
```{r}
sentences %>%
  head(5) %>% 
  str_split(" ", simplify = TRUE)
```
request a maximum number of pieces
```{r}
fields <- c("Name: Hadley", "Country: NZ", "Age: 35")
fields %>% str_split(": ", n = 2, simplify = TRUE)
```
Instead of splitting up strings by patterns, you can also split up by character, line, sentence and word `boundary()`s:
```{r}
x <- "This is a sentence.  This is another sentence."
str_view_all(x, boundary("word"))

str_split(x, " ")[[1]]
str_split(x, boundary("word"))[[1]]
```
#### 2.2.1.4 Substitute
##### {base}
###### sub() and gsub()
```{r}
testName <- "this_is_a_test"

# sub only the first one
sub("_","",testName)

# sub all
gsub("_","",testName)
```
#### 2.2.1.5 Number of characters
##### {base}
```{r}
nchar("Jeffrey Leek")
```
##### {stringr}
```{r}
str_length(c("a", "R for data science", NA))
```
#### 2.2.1.6 Extract parts of the string / subset strings
##### {base}
```{r}
substr("Jeffrey Leek",1,7) # 1st to 7th characters
```
##### {stringr}
###### str_sub()
```{r}
x <- c("Apple", "Banana", "Pear")
str_sub(x, 1, 3)
# negative numbers count backwards from end
str_sub(x, -3, -1)
```
takes `start` and `end` arguments which give the (inclusive) position of the substring
```{r}
str_sub("a", 1, 5)
```
won't fail if the string is too short: it will just return as much as possible
```{r}
str_sub(x, 1, 1) <- str_to_lower(str_sub(x, 1, 1))
x
```
can also use the assignment form to modify strings
#### 2.2.1.7 Combine strings
##### {base}
```{r}
# paste to get a string spereated with a space
paste("Jeffrey","Leek")
# paste to get a string without space
paste0("Jeffrey","Leek")
```
##### {stringr}
###### str_c()
```{r}
str_c("x", "y")
str_c("x", "y", sep = ", ") 

str_c("prefix-", c("a", "b", "c"), "-suffix")
#> [1] "prefix-a-suffix" "prefix-b-suffix" "prefix-c-suffix"
```
Use the `sep` argument to control how they're separated
```{r}
str_c(c("x", "y", "z"), collapse = ", ")
#> [1] "x, y, z"
```
To collapse a vector of strings into a single string, use `collapse`
```{r}
name <- "Hadley"
time_of_day <- "morning"
birthday <- FALSE

str_c(
  "Good ", time_of_day, " ", name,
  if (birthday) " and HAPPY BIRTHDAY",
  "."
)
```
Objects of length 0 are silently dropped. This is particularly useful in conjunction with `if`
#### 2.2.1.8 Trim whitespace
##### {stringr}
```{r}
# removes whitespace from start and end of string
str_trim("Jeff      ")
```
#### 2.2.1.9 Regular expression
##### 2.2.1.9.1 Basic matches
`.` is used to refer to any character
```{r}
9.11
```
##### 2.2.1.9.2 Anchors
`^` represents the start of a line
```{r}
^i think
```
`$` represents the end of a line
```{r}
morning$
```
You can also match the boundary between words with `\b`. You can search for `\bsum\b` to avoid matching `summarise`, `summary`, `rowsum` and so on.  
##### 2.2.1.9.3 Character Classes with [] and alternatives
- `\d`: matches any digit.
- `\s`: matches any whitespace (e.g. space, tab, newline).
- `[abc]`: matches a, b, or c.
- `[^abc]`: matches anything except a, b, or c.
A character class containing a single character is a nice alternative to backslash escapes when you want to include a single metacharacter in a regex. Many people find this more readable.
```{r}
# Look for a literal character that normally has special meaning in a regex
str_view(c("abc", "a.c", "a*c", "a c"), "a[.]c")
str_view(c("abc", "a.c", "a*c", "a c"), ".[*]c")
str_view(c("abc", "a.c", "a*c", "a c"), "a[ ]")
```
This works for most (but not all) regex metacharacters: `$` `.` `|` `?` `*` `+` `(` `)` `[` `{`. Unfortunately, a few characters have special meaning even inside a character class and must be handled with backslash escapes: `]` `\` `^` and `-`.  
`|` "or"
```{r}
flood|fire
flood|earthquake|hurricane|coldfire
str_view(c("grey", "gray"), "gr(e|a)y")
```
list a set of characters we will accept at a given point in the match
```{r}
[Bb][Uu][Ss][Hh]

^[Ii] am

^[0-9][a-zA-Z]

^[Gg]ood|[Bb]ad
^([Gg]ood|[Bb]ad)
```
When used at the beginning of a character class, the "^" is also a metacharacter and indicates matching characters NOT in the indicated class
```{r}
[^?.]$
```
##### 2.2.1.9.4 Repetition
- `?`: 0 or 1
- `+`: 1 or more
- `*`: 0 or more
```{r}
x <- "1888 is the longest year in Roman numerals: MDCCCLXXXVIII"
str_view(x, "CC?")
str_view(x, "CC+")
str_view(x, 'C[LX]+')
```

```{r}
[Gg]eorge( [Ww]\.)? [Bb]ush
```

```{r}
(.*)
[0-9]+ (.*)[0-9]+
```
`{}` are referred to as interval quantifiers which specify the minimum and maximum number of matches of an expression
```{r}
[Bb]ush ([^ ]+ +){1,5}debate
```
specify the number of matches precisely:
- `{n}`: exactly n
- `{n,}`: n or more
- `{,m}`: at most m
- `{n,m}`: between n and m
```{r}
str_view(x, "C{2}")
str_view(x, "C{2,}")
str_view(x, "C{2,3}")
```
The greediness of `*` (longest possible match) can be turned off with the `?`
```{r}
^s(.*?)s$
str_view(x, 'C{2,3}?')
str_view(x, 'C[LX]+?')
```
##### 2.2.1.9.5 Grouping and backreferences
`()` not only limits the scope of alternatives divided by a `|`, but also can be used to "remember" text matched by the subexpression enclosed; We refer to the matched text with `\1`, `\2`, etc.
```{r}
+([a-zA-Z]+) +\1 +
str_view(fruit, "(..)\\1", match = TRUE)
```
##### 2.2.1.9.6 Other uses of regular expressions
###### apropos()
searches all objects available from the global environment
```{r}
apropos("replace")
```
###### dir()
lists all the files in a directory. The `pattern` argument takes a regular expression and only returns file names that match the pattern.
```{r}
# find all the R Markdown files in the current directory
head(dir(pattern = "\\.Rmd$"))
```
#### 2.2.1.10 Detect matches
##### {base}
###### grep() and grepl()
```{r}
grep("Alameda",cameraData$intersection)
table(grepl("Alameda",cameraData$intersection))
grep("Alameda",cameraData$intersection,value=TRUE)

# subset
cameraData2 <- cameraData[!grepl("Alameda",cameraData$intersection),]
```
`pattern="Alameda"` character string containing a regular expression (or character string for fixed = TRUE) to be matched
- `grep(value = FALSE)` returns a vector of the indices of the elements of x that yielded a match (or not, for `invert = TRUE`)
- `grep(value = TRUE)` returns a character vector containing the selected elements of x
- `grepl` returns a logical vector (match or not for each element of x)
##### {stringr}
###### str_detect()
```{r}
x <- c("apple", "banana", "pear")
str_detect(x, "e")
```
returns a logical vector the same length as the input
```{r}
# How many common words start with t?
sum(str_detect(words, "^t"))
#> [1] 65
# What proportion of common words end with a vowel?
mean(str_detect(words, "[aeiou]$"))
#> [1] 0.277
```
###### str_detect() or str_subset(): Logical subsetting
select the elements that match a pattern
```{r}
words[str_detect(words, "x$")]
#> [1] "box" "sex" "six" "tax"
str_subset(words, "x$")
#> [1] "box" "sex" "six" "tax"

df %>% 
  filter(str_detect(words, "x$"))
```
###### str_count()
how many matches are there in a string
```{r}
x <- c("apple", "banana", "pear")
str_count(x, "a")
#> [1] 1 3 1

# On average, how many vowels per word?
mean(str_count(words, "[aeiou]"))
#> [1] 1.99
```
#### 2.2.1.11 Extract matches
##### {stringr}
###### str_extract()
extract the actual text of a match
```{r}
colours <- c("red", "orange", "yellow", "green", "blue", "purple")
colour_match <- str_c(colours, collapse = "|")
has_colour <- str_subset(sentences, colour_match)
matches <- str_extract(has_colour, colour_match)
```
###### str_extract_all() 
get all matches
```{r}
more <- sentences[str_count(sentences, colour_match) > 1]
str_view_all(more, colour_match)
str_extract_all(more, colour_match)
```
`simplify = TRUE`: return a matrix with short matches expanded to the same length as the longest
```{r}
str_extract_all(more, colour_match, simplify = TRUE)

x <- c("a", "a b", "a b c")
str_extract_all(x, "[a-z]", simplify = TRUE)
```
###### str_match()
gives each individual component. Instead of a character vector, it returns a matrix, with one column for the complete match followed by one column for each group:
```{r}
noun <- "(a|the) ([^ ]+)"

has_noun <- sentences %>%
  str_subset(noun)
has_noun %>% 
  str_match(noun)
str_match_all()
```
##### {tidyr}
###### extract() 
If your data is in a tibble. It works like `str_match()` but requires you to name the matches, which are then placed in new columns:
```{r}
tibble(sentence = sentences) %>% 
  tidyr::extract(
    sentence, c("article", "noun"), "(a|the) ([^ ]+)", 
    remove = FALSE
  )
```
#### 2.2.1.12 Replace matches
##### {stringr}
###### str_replace(), str_replace_all()
replace matches with new strings
```{r}
# replace a pattern with a fixed string
x <- c("apple", "pear", "banana")
str_replace(x, "[aeiou]", "-")
str_replace_all(x, "[aeiou]", "-")
```
With `str_replace_all()` you can perform multiple replacements by supplying a named vector:
```{r}
x <- c("1 house", "2 cars", "3 people")
str_replace_all(x, c("1" = "one", "2" = "two", "3" = "three"))
#> [1] "one house"    "two cars"     "three people"
```
Instead of replacing with a fixed string you can use backreferences to insert components of the match.
```{r}
# flip the order of the second and third words
sentences %>% 
  str_replace("([^ ]+) ([^ ]+) ([^ ]+)", "\\1 \\3 \\2")
```
#### 2.2.1.13 Other types of pattern
##### {base}
###### regex()
When you use a pattern that's a string, it's automatically wrapped into a call to `regex()`
```{r}
bananas <- c("banana", "Banana", "BANANA")
str_view(bananas, regex("banana", ignore_case = TRUE))
```
`ignore_case = TRUE` allows characters to match either their uppercase or lowercase forms. This always uses the current locale.
```{r}
x <- "Line 1\nLine 2\nLine 3"
str_extract_all(x, "^Line")[[1]]
#> [1] "Line"
str_extract_all(x, regex("^Line", multiline = TRUE))[[1]]
#> [1] "Line" "Line" "Line"
```
`multiline = TRUE` allows `^` and `$` to match the start and end of each line rather than the start and end of the complete string.
```{r}
phone <- regex("
  \\(?     # optional opening parens
  (\\d{3}) # area code
  [) -]?   # optional closing parens, space, or dash
  (\\d{3}) # another three numbers
  [ -]?    # optional space or dash
  (\\d{3}) # three more numbers
  ", comments = TRUE)

str_match("514-791-8141", phone)
```
`comments = TRUE` allows you to use comments and white space to make complex regular expressions more understandable. Spaces are ignored, as is everything after `#`. To match a literal space, you'll need to escape it: `"\\ "`.
`dotall = TRUE` allows `.` to match everything, including `\n`.
###### fixed()
matches exactly the specified sequence of bytes. It ignores all special regular expressions and operates at a very low level. This allows you to avoid complex escaping and can be much faster than regular expressions.
###### coll()
compare strings using standard **coll**ation rules. This is useful for doing case insensitive matching. Note that `coll()` takes a locale parameter that controls which rules are used for comparing characters.
```{r}
a1 <- "\u00e1"
a2 <- "a\u0301"
c(a1, a2)
#> [1] "á" "á"
a1 == a2
#> [1] FALSE

# use coll() to respect human character comparison rules
str_detect(a1, fixed(a2))
#> [1] FALSE
str_detect(a1, coll(a2))
#> [1] TRUE
```

```{r}
# That means you also need to be aware of the difference
# when doing case insensitive matches:
i <- c("I", "İ", "i", "ı")
i
#> [1] "I" "İ" "i" "ı"

str_subset(i, coll("i", ignore_case = TRUE))
#> [1] "I" "i"
str_subset(i, coll("i", ignore_case = TRUE, locale = "tr"))
#> [1] "İ" "i"
```
because the rules for recognising which characters are the same are complicated, `coll()` is relatively slow compared to `regex()` and `fixed()`.
### 2.2.2 Factor
#### 2.2.2.1 Create factor variables
##### 2.2.2.1.1 {base}
```{r}
restData$zcf <- factor(restData$zipCode)
```

```{r}
restData$zipGroups = cut(restData$zipCode,breaks=quantile(restData$zipCode))
```
apply `cut()` to quantitative variable (zipCode). `breaks=` break up according to a list of value (the quantiles to that zip code). Returns a factor variable.
```{r}
yesnofac = factor(yesno,levels=c("yes","no"))
relevel(yesnofac,ref="no")
```
if no `levels=c("yes","no")`, by default, `factor()` treats the lowest value alphabetically as the first factor variable.
```{r}
x1 <- c("Dec", "Apr", "Jan", "Mar")
x2 <- c("Dec", "Apr", "Jam", "Mar")
month_levels <- c(
  "Jan", "Feb", "Mar", "Apr", "May", "Jun", 
  "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"
)
```
And any values not in the set will be silently converted to NA
```{r}
y2 <- factor(x2, levels = month_levels)
```
If you want a warning, you can use `readr::parse_factor()`
```{r}
y2 <- parse_factor(x2, levels = month_levels)
```
Sometimes you'd prefer that the order of the levels match the order of the first appearance in the data. You can do that when creating the factor by setting levels to `unique(x)`, or after the fact, with `forcats::fct_inorder()`:
```{r}
f1 <- factor(x1, levels = unique(x1))
f2 <- x1 %>% factor() %>% fct_inorder()
```
##### 2.2.2.1.2 {Hmisc}
###### cut2()
```{r}
library(Hmisc)
restData$zipGroups = cut2(restData$zipCode,g=4)
```
`g=` number of quantile groups
#### 2.2.2.2 Levels of factor variables
If you ever need to access the set of valid levels directly, you can do so with `levels()`:
```{r}
levels(f2)
```
When factors are stored in a tibble, you can't see their levels so easily. One way to see them is with `count()`:
```{r}
gss_cat %>%
  count(race)
```
Or with a bar chart:
```{r}
ggplot(gss_cat, aes(race)) +
  geom_bar()
```
By default, ggplot2 will drop levels that don't have any values. You can force them to display with:
```{r}
ggplot(gss_cat, aes(race)) +
  geom_bar() +
  scale_x_discrete(drop = FALSE)
```
#### 2.2.2.3 Modify factor order
##### {base}
```{r}
schoolPub$High.Grade.=factor(schoolPub$High.Grade.,
                             levels = levelCat,
                             labels = levelCat,
                             ordered = T)
```
##### {forcats}
###### fct_reorder()
```{r}
ggplot(relig_summary, aes(tvhours, fct_reorder(relig, tvhours))) +
  geom_point()

relig_summary %>%
  mutate(relig = fct_reorder(relig, tvhours)) %>%
  ggplot(aes(tvhours, relig)) +
    geom_point()
```
- `f`, the factor whose levels you want to modify.
- `x`, a numeric vector that you want to use to reorder the levels.
- Optionally, `fun`, a function that's used if there are multiple values of `x` for each value of `f`. The default value is `median`.
###### fct_relevel()
takes a factor, `f`, and then any number of levels that you want to move to the front of the line.
```{r}
ggplot(rincome_summary, aes(age, fct_relevel(rincome, "Not applicable"))) +
  geom_point()
```
###### fct_reorder2()
useful when you are colouring the lines on a plot. `fct_reorder2()` reorders the factor by the `y` values associated with the largest `x` values. This makes the plot easier to read because the line colours line up with the legend.
```{r}
by_age <- gss_cat %>%
  filter(!is.na(age)) %>%
  count(age, marital) %>%
  group_by(age) %>%
  mutate(prop = n / sum(n))

ggplot(by_age, aes(age, prop, colour = marital)) +
  geom_line(na.rm = TRUE)

ggplot(by_age, aes(age, prop, colour = fct_reorder2(marital, age, prop))) +
  geom_line() +
  labs(colour = "marital")
```
###### fct_infreq()
order levels in increasing frequency: this is the simplest type of reordering because it doesn't need any extra variables. You may want to combine with `fct_rev()`
```{r}
gss_cat %>%
  mutate(marital = marital %>% fct_infreq() %>% fct_rev()) %>%
  ggplot(aes(marital)) +
    geom_bar()
```
#### 2.2.2.4 Modify factor levels
##### {forcats}
###### fct_recode()
allows you to recode, or change, the value of each level.
```{r}
gss_cat %>% count(partyid)
gss_cat %>%
  mutate(partyid = fct_recode(partyid,
    "Republican, strong"    = "Strong republican",
    "Republican, weak"      = "Not str republican",
    "Independent, near rep" = "Ind,near rep",
    "Independent, near dem" = "Ind,near dem",
    "Democrat, weak"        = "Not str democrat",
    "Democrat, strong"      = "Strong democrat"
  )) %>%
  count(partyid)
```
To combine groups, you can assign multiple old levels to the same new level:
```{r}
gss_cat %>%
  mutate(partyid = fct_recode(partyid,
    "Republican, strong"    = "Strong republican",
    "Republican, weak"      = "Not str republican",
    "Independent, near rep" = "Ind,near rep",
    "Independent, near dem" = "Ind,near dem",
    "Democrat, weak"        = "Not str democrat",
    "Democrat, strong"      = "Strong democrat",
    "Other"                 = "No answer",
    "Other"                 = "Don't know",
    "Other"                 = "Other party"
  )) %>%
  count(partyid)
```
###### fct_collapse()
a useful variant of `fct_recode()`; collapse a lot of levels
```{r}
gss_cat %>%
  mutate(partyid = fct_collapse(partyid,
    other = c("No answer", "Don't know", "Other party"),
    rep = c("Strong republican", "Not str republican"),
    ind = c("Ind,near rep", "Independent", "Ind,near dem"),
    dem = c("Not str democrat", "Strong democrat")
  )) %>%
  count(partyid)
```
###### fct_lump()
lump together all the small groups to make a plot or table simpler
```{r}
gss_cat %>%
  mutate(relig = fct_lump(relig)) %>%
  count(relig)
```
The default behaviour is to progressively lump together the smallest groups, ensuring that the aggregate is still the smallest group.
```{r}
gss_cat %>%
  mutate(relig = fct_lump(relig, n = 10)) %>%
  count(relig, sort = TRUE) %>%
  print(n = Inf)
```
`n`: specify how many groups (excluding other) to keep
#### 2.2.2.5 Further resources
- A nice [lecture on categorical and factor variables](https://www.stat.berkeley.edu/classes/s133/factors.html)
- [Wrangling categorical data in R](https://peerj.com/preprints/3163/)
### 2.2.3 Date and time
#### 2.2.3.1 {base}
##### format date
- `%d` day as number (0-31)
- `%a` abbreviated weekday
- `%A` unabbreviated weekday
- `%m` month (00-12)
- `%b` abbreviated month
- `%B` unabbrevidated month
- `%y` 2 digit year
- `%Y` four digit year
```{r}
format(Sys.Date(),"%a %b %d")
```
##### create date
```{r}
x = c("1jan1960", "2jan1960", "31mar1960", "30jul1960")
z = as.Date(x, "%d%b%Y")
z[1] - z[2]
as.numeric(z[1]-z[2])
```
##### attribute of date
```{r}
weekdays(d2)
months(d2)

# days from origin
julian(d2) 
```
#### 2.2.3.2 {lubridate}
##### 2.2.3.2.1 Create date/time
1. From strings
```{r}
mdy("08/04/2013")
dmy("03-04-2013")
ymd(20170131)

ymd("2017-01-31")
mdy("January 31st, 2017")
dmy("31-Jan-2017")

ymd_hms("2011-08-03 10:15:03")
ymd_hms("2011-08-03 10:15:03",tz="Pacific/Auckland")

x = dmy(c("1jan2013", "2jan2013", "31mar2013", "30jul2013"))
```
2. From individual components
`make_date()`, `make_datetime()`
```{r}
flights %>% 
  select(year, month, day, hour, minute) %>% 
  mutate(departure = make_datetime(year, month, day, hour, minute))
```
when you use date-times in a numeric context (like in a histogram), 1 means 1 second, so a binwidth of 86400 means one day. For dates, 1 means 1 day.
```{r}
flights_dt %>% 
  ggplot(aes(dep_time)) + 
  geom_freqpoly(binwidth = 86400) # 86400 seconds = 1 day

flights_dt %>% 
  filter(dep_time < ymd(20130102)) %>% 
  ggplot(aes(dep_time)) + 
  geom_freqpoly(binwidth = 600) # 600 s = 10 minutes
```
3. From other types
```{r}
as_datetime(today())
as_date(now())
```
Sometimes you'll get date/times as numeric offsets from the "Unix Epoch", 1970-01-01. If the offset is in seconds, use as_datetime(); if it's in days, use as_date().
```{r}
as_datetime(60 * 60 * 10)
#> [1] "1970-01-01 10:00:00 UTC"
as_date(365 * 10 + 2)
#> [1] "1980-01-01"
```
##### 2.2.3.2.2 Date-time components
###### Get components
- `year()`
- `month()`
- `mday()` (day of the month)
- `yday()` (day of the year)
- `wday()` (day of the week)
- `hour()`
- `minute()`
- `second()`
```{r}
# weekday
wday(x[1])
# show weekday abbreviation
wday(x[1],label=TRUE)
```
For `month()` and `wday()` you can set `label = TRUE` to return the abbreviated name of the month or day of the week. Set `abbr = FALSE` to return the full name.
```{r}
datetime <- ymd_hms("2016-07-08 12:34:56")

month(datetime, label = TRUE)
wday(datetime, label = TRUE, abbr = FALSE)
```
###### Rounding
`floor_date()`, `round_date()`, and `ceiling_date()`. Each function takes a vector of dates to adjust and then the name of the unit round down (floor), round up (ceiling), or round to.
```{r}
flights_dt %>% 
  count(week = floor_date(dep_time, "week")) %>% 
  ggplot(aes(week, n)) +
    geom_line()
```
###### Set components
```{r}
(datetime <- ymd_hms("2016-07-08 12:34:56"))
year(datetime) <- 2020
month(datetime) <- 01
hour(datetime) <- hour(datetime) + 1
```
`update()` allows you to set multiple values at once.
```{r}
update(datetime, year = 2020, month = 2, mday = 2, hour = 2)
```
If values are too big, they will roll-over:
```{r}
ymd("2015-02-01") %>% 
  update(mday = 30)
ymd("2015-02-01") %>% 
  update(hour = 400)
```
use `update()` to show the distribution of flights across the course of the day for every day of the year:
```{r}
flights_dt %>% 
  mutate(dep_hour = update(dep_time, yday = 1)) %>% 
  ggplot(aes(dep_hour)) +
    geom_freqpoly(binwidth = 300)
```
Setting larger components of a date to a constant is a powerful technique that allows you to explore patterns in the smaller components.
##### 2.2.3.2.3 Time spans
###### duration
```{r}
h_age <- today() - ymd(19791014)
as.duration(h_age)
```
Durations always record the time span in seconds. Larger units are created by converting minutes, hours, days, weeks, and years to seconds at the standard rate (60 seconds in a minute, 60 minutes in an hour, 24 hours in day, 7 days in a week, 365 days in a year).
```{r}
dseconds(15)
#> [1] "15s"
dminutes(10)
#> [1] "600s (~10 minutes)"
dhours(c(12, 24))
#> [1] "43200s (~12 hours)" "86400s (~1 days)"
ddays(0:5)
#> [1] "0s"                "86400s (~1 days)"  "172800s (~2 days)"
#> [4] "259200s (~3 days)" "345600s (~4 days)" "432000s (~5 days)"
dweeks(3)
#> [1] "1814400s (~3 weeks)"
dyears(1)
#> [1] "31536000s (~52.14 weeks)"
```

```{r}
2 * dyears(1)
dyears(1) + dweeks(12) + dhours(15)
tomorrow <- today() + ddays(1)
last_year <- today() - dyears(1)
```
###### Periods
Periods are time spans but don't have a fixed length in seconds, instead they work with "human" times, like days and months. That allows them work in a more intuitive way:
```{r}
one_pm
#> [1] "2016-03-12 13:00:00 EST"
one_pm + days(1)
#> [1] "2016-03-13 13:00:00 EDT"
```

```{r}
seconds(15)
minutes(10)
hours(c(12, 24))
days(7)
months(1:6)
weeks(3)
years(1)

10 * (months(6) + days(1))
days(50) + hours(25) + minutes(2)
```
Compared to durations, periods are more likely to do what you expect:
```{r}
# A leap year
ymd("2016-01-01") + dyears(1)
#> [1] "2016-12-31"
ymd("2016-01-01") + years(1)
#> [1] "2017-01-01"

# Daylight Savings Time
one_pm + ddays(1)
#> [1] "2016-03-13 14:00:00 EDT"
one_pm + days(1)
#> [1] "2016-03-13 13:00:00 EDT"
```
###### Intervals
An **interval** is a duration with a starting point: that makes it precise so you can determine exactly how long it is:
```{r}
next_year <- today() + years(1)
(today() %--% next_year) / ddays(1)
```
To find out how many periods fall into an interval, you need to use integer division:
```{r}
(today() %--% next_year) %/% days(1)
```
###### Summary
If you only care about physical time, use a duration; if you need to add human times, use a period; if you need to figure out how long a span is in human units, use an interval.
##### 2.2.3.2.4 Time zones
In R, the time zone is an attribute of the date-time that only controls printing. For example, these three objects represent the same instant in time:
```{r}
(x1 <- ymd_hms("2015-06-01 12:00:00", tz = "America/New_York"))
(x2 <- ymd_hms("2015-06-01 18:00:00", tz = "Europe/Copenhagen"))
(x3 <- ymd_hms("2015-06-02 04:00:00", tz = "Pacific/Auckland"))

#> Time difference of 0 secs
x1 - x2
x1 - x3
```
Unless otherwise specified, lubridate always uses UTC. UTC (Coordinated Universal Time) does not have DST, which makes a convenient representation for computation. Operations that combine date-times, like c(), will often drop the time zone. In that case, the date-times will display in your local time zone:
```{r}
x4 <- c(x1, x2, x3)
```
##### Change the time zone
1. Keep the instant in time the same, and change how it's displayed. Use this when the instant is correct, but you want a more natural display.
```{r}
x4a <- with_tz(x4, tzone = "Australia/Lord_Howe")

# no time difference
x4a - x4
```
2. Change the underlying instant in time. Use this when you have an instant that has been labelled with the incorrect time zone, and you need to fix it.
```{r}
x4b <- force_tz(x4, tzone = "Australia/Lord_Howe")

x4b - x4
#> Time differences in hours
#> [1] -14.5 -14.5 -14.5
```
##### 2.2.3.2.5 Further resources and notes
- More information in this nice [lubridate tutorial](http://www.r-statistics.com/2012/03/do-more-with-dates-and-times-in-r-with-lubridate-1-1-0/)
- The [lubridate vignette](http://cran.r-project.org/web/packages/lubridate/vignettes/lubridate.html) is the same content
- Ultimately you want your dates and times as class "Date" or the classes "POSIXct", "POSIXlt". For more information type `?POSIXlt`
- POSIXct's are always easier to work with, so if you find you have a POSIXlt, you should always convert it to a regular date time `lubridate::as_date_time()`.
## 2.3 Further resources
- [Data Wrangling with {dplyr} and {tidyr} Cheat Sheet](https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf)
- Andrew Jaffe's [lecture notes](http://www.biostat.jhsph.edu/~ajaffe/lec_winterR/Lecture%202.pdf)

# 3. Step 3 Exploratory Data Analysis
## 3.1 Summarize the data
### 3.1.1 Basic summary
#### {base}
Look at a bit of the data
```{r}
head(restData,n=3)
tail(restData,n=3)
```
Five Number Summary
```{r}
summary(restData)
summary(pollution$pm25)
```
More in depth information
```{r}
str(restData)
```
### 3.1.2 Distribution
#### 3.1.2.1 One dimension
##### {base}
###### Quantiles of quantitative variables
```{r}
quantile(restData$councilDistrict,na.rm=TRUE)
quantile(restData$councilDistrict,probs=c(0.5,0.75,0.9))
```
###### table()
```{r}
table(restData$zipCode,useNA="ifany")
table(restData$councilDistrict,restData$zipCode)
```
`useNA="ifany"` if there are any missing values, they'll be an added column to this table, which will be NA, and it'll tell you the number of missing values there is.
###### Boxplot
```{r}
boxplot(pollution$pm25, col = "blue")
```
###### Histogram
```{r}
hist(pollution$pm25, col = "green", breaks = 100)
rug(pollution$pm25)
```
`rug()` plots all of the points in your dataset along underneath the histogram  
`breaks=100` the number of bars in the histogram
###### Overlaying Features
```{r}
abline(h = 12)
abline(v = median(pollution$pm25), col = "magenta", lwd = 4)
```
`h=` horizontal line  
`v=` vertical line  
`lwd=`line width
###### Barplot
```{r}
barplot(table(pollution$region), col = "wheat", main = "Number of Counties in Each Region")
```
#### 3.1.2.2 Two dimensions
##### {base}
###### Multiple Boxplots
```{r}
boxplot(pm25 ~ region, data = pollution, col = "red")
```
`pm25 ~ region` `pm25` is a numeric vector of data values to be split into groups according to the grouping variable `region` (usually a factor)
###### Multiple Histograms
```{r}
par(mfrow = c(2, 1), mar = c(4, 4, 2, 1))
hist(subset(pollution, region == "east")$pm25, col = "green")
hist(subset(pollution, region == "west")$pm25, col = "green")
```
`par()` set or query graphical parameters  
`mar=` A numerical vector of the form c(bottom, left, top, right) which gives the number of lines of margin to be specified on the four sides of the plot.  
`mfrow=` A vector of the form c(nr, nc). Subsequent figures will be drawn in an nr-by-nc array on the device by columns (mfcol), or rows (mfrow), respectively.
###### Scatterplot
```{r}
with(pollution, plot(latitude, pm25, col = region))
abline(h = 12, lwd = 2, lty = 2)
```
`col=region` break out color groups by region  
`lty` line type
###### Multiple Scatterplots
```{r}
par(mfrow = c(1, 2), mar = c(5, 4, 2, 1))
with(subset(pollution, region == "west"), plot(latitude, pm25, main = "West"))
with(subset(pollution, region == "east"), plot(latitude, pm25, main = "East"))
```
### 3.1.3 Check for missing values
#### {mice}
md.pattern()
```{r}
md.pattern(bc_data, plot = FALSE)
```
#### {base}
```{r}
sum(is.na(restData$councilDistrict))
any(is.na(restData$councilDistrict))
all(restData$zipCode > 0)
```
Row and column sums
```{r}
colSums(is.na(restData))
all(colSums(is.na(restData))==0)
```
### 3.1.4 Values with specific characteristics
#### {base}
```{r}
table(restData$zipCode %in% c("21212","21213"))
```
##### Cross tables
```{r}
xt <- xtabs(Freq ~ Gender + Admit,data=DF)
```
`Freq` the variable that you want to be displayed in the table  
`Gender + Admit` break that down by variables
```{r}
xt = xtabs(breaks ~.,data=warpbreaks)
```
`~.` break down by all the variables in the data set
##### Flat tables
```{r}
ftable(xt)
```
`ftable` make flat tables from the crosstabs. It will summarize the data in a much smaller, more compact form. So it's easier to see.
### 3.1.5 Size of a data set
#### {base}
```{r}
object.size(fakeData)
print(object.size(fakeData),units="Mb")
```
## 3.2 Plotting
### 3.2.1 {base}
#### 3.2.1.1 Parameters
Many base plotting functions share a set of parameters. Here are a few key ones:
- `pch`: the plotting symbol (default is open circle)
- `lty`: the line type (default is solid line), can be dashed, dotted, etc.
- `lwd`: the line width, specified as an integer multiple
- `col`: the plotting color, specified as a number, string, or hex code; the colors() function gives you a vector of colors by name
- `xlab`: character string for the x-axis label
- `ylab`: character string for the y-axis label
##### par(): specify global graphics parameters
These parameters can be overridden when specified as arguments to specific plotting functions. (Use `dev.off` or `plot.new` to reset to the defaults.)
- `las`: the orientation of the axis labels on the plot
- `bg`: the background color
- `mar`: the margin size
- `oma`: the outer margin size (default is 0 for all sides)
- `mfrow`: number of plots per row, column (plots are filled row-wise)
- `mfcol`: number of plots per row, column (plots are filled column-wise)
#### 3.2.1.2 Plotting functions
- `plot`: make a scatterplot, or other type of plot depending on the class of the object being plotted
- `lines`: add lines to a plot, given a vector x values and a corresponding vector of y values (or a 2-column matrix); this function just connects the dots
- `points`: add points to a plot
- `text`: add text labels to a plot using specified x, y coordinates
- `title`: add annotations to x, y axis labels, title, subtitle, outer margin
- `mtext`: add arbitrary text to the margins (inner or outer) of the plot
- `axis`: adding axis ticks/labels
##### With annotations
```{r}
library(datasets)
with(airquality, plot(Wind, Ozone))
title(main = "Ozone and Wind in New York City")  # Add a title
```
a subset of blue points corresponding to the data points in the month of May
```{r}
with(airquality, plot(Wind, Ozone, main = "Ozone and Wind in New York City"))
with(subset(airquality, Month == 5), points(Wind, Ozone, col = "blue"))
```

```{r}
with(airquality, plot(Wind, Ozone, main = "Ozone and Wind in New York City", type = "n"))
with(subset(airquality, Month == 5), points(Wind, Ozone, col = "blue"))
with(subset(airquality, Month != 5), points(Wind, Ozone, col = "red"))
legend("topright", pch = 1, col = c("blue", "red"), legend = c("May", "Other Months"))
```
`type="n"`: sets up the plot and initializes the graphics device but it doesn't actually plot anything.
###### Annotating functions
- `title()`: include x- and y- axis labels, title, subtitle, and outer margin.
- `mtext()`: adds arbitrary text to either the outer or inner margins of the plot
- `axis()`: adds axis ticks and labels
- `legend()`: explains to the reader what the symbols your plot uses mean.
##### With regression line
```{r}
with(airquality, plot(Wind, Ozone, main = "Ozone and Wind in New York City", pch = 20))
model <- lm(Ozone ~ Wind, airquality)
abline(model, lwd = 2)
```
##### Multiple plots
```{r}
par(mfrow = c(1, 2))
with(airquality, {
	plot(Wind, Ozone, main = "Ozone and Wind")
	plot(Solar.R, Ozone, main = "Ozone and Solar Radiation")
})
```

```{r}
par(mfrow = c(1, 3), mar = c(4, 4, 2, 1), oma = c(0, 0, 2, 0))
with(airquality, {
	plot(Wind, Ozone, main = "Ozone and Wind")
	plot(Solar.R, Ozone, main = "Ozone and Solar Radiation")
	plot(Temp, Ozone, main = "Ozone and Temperature")
	mtext("Ozone and Weather in New York City", outer = TRUE)
})
```
#### 3.2.1.3 Create plots and send it to file device
1. Explicitly launch a graphics device
2. Call a plotting function to make a plot (Note: if you are using a file device, no plot will appear on the screen)
3. Annotate plot if necessary
4. Explicitly close graphics device with dev.off() (this is very important!)
```{r}
pdf(file = "myplot.pdf")  ## Open PDF device; create 'myplot.pdf' in my working directory
## Create plot and send to a file (no plot appears on screen)
with(faithful, plot(eruptions, waiting))  
title(main = "Old Faithful Geyser data")  ## Annotate plot; still nothing on screen
dev.off()  ## Close the PDF file device
## Now you can view the file 'myplot.pdf' on your computer
```
#### 3.2.1.4 Open Multiple Graphics Devices
It is possible to open multiple graphics devices (screen, file, or both), for example when viewing multiple plots at once. Plotting can only occur on one graphics device at a time. The currently active graphics device can be found by calling `dev.cur()`. Every open graphics device is assigned an integer >= 2. You can change the active graphics device with `dev.set(<integer>)` where `<integer>` is the number associated with the graphics device you want to switch to.
#### 3.2.1.5 Copy Plots
Copying a plot to another device can be useful because some plots require a lot of code and it can be a pain to type all that in again for a different device.  
`dev.copy`: copy a plot from one device to another  
`dev.copy2pdf`: specifically copy a plot to a PDF file  
NOTE: Copying a plot is not an exact operation, so the result may not be identical to the original.
```{r}
library(datasets)
with(faithful, plot(eruptions, waiting))  ## Create plot on screen device
title(main = "Old Faithful Geyser data")  ## Add a main title
dev.copy(png, file = "geyserplot.png")  ## Copy my plot to a PNG file
dev.off()  ## Don't forget to close the PNG device!
```
#### 3.2.1.6 Colors
- `colors()`: lists the names of 657 predefined colors you can use in any plotting function.
- `colorRamp()`: takes a palette of colors (the arguments) and returns a function that takes values between 0 and 1 as arguments. The 0 and 1 correspond to the extremes of the color palette. Arguments between 0 and 1 return blends of these extremes.
```{r}
pal <- colorRamp(c("red","blue"))
pal(0)
pal(seq(0,1,len=6))
```
- `colorRampPalette()`: takes a palette of colors and returns a function. This function takes integer arguments (instead of numbers between 0 and 1) and returns a vector of colors each of which is a blend of colors of the original palette. The argument you pass to the returned function specifies the number of colors you want returned.
```{r}
p1 <- colorRampPalette(c("red","blue"))
p1(2)
p3 <- colorRampPalette(c("blue","green"),alpha=T)
```
- `smoothScatter()`: creates a 2D histogram of the points in your plot using a certain set of colors. The default set of colors is the blues palette in the RColorBrewer package.
```{r}
smoothScatter(x,y)
```
#### 3.2.1.7 Summary
- Vector formats (pdf,svg) are good for line drawings and plots with solid colors using a modest number of points
  - pdf: useful for line-type graphics and papers. Resizes well, portable, not efficient if a plot has many objects/points.
  - svg: XML-based, scalable vector graphics. Supports animation and interactivity and is useful for web-based plots.
- Bitmap formats (png,jpeg,tiff,bmp) are good for plots with a large number of points, natural scenes or web-based plots
  - png (Portable Network Graphics): good for line drawings or images with solid colors. Uses lossless compression (like the old GIF format), most web browsers can read this format natively. Is good for plots with many points, but does not resize well.
  - jpeg: good for photographs or natural scenes. Use lossy compression, so good for plots with many points. Don't resize well, but can be read by almost any computer and any web browser. Not great for line drawings.

### 3.2.2 {lattice}
lattice plots are created with a single function call such as `xyplot()` or `bwplot()`. is most useful for conditioning types of plots which display how y changes with x across levels of z. The variable z might be a categorical variable of your data. Also good for putting many plots on a screen at once.
#### 3.2.2.1 Plotting Functions
- `xyplot`: main function for creating scatterplots
- `bwplot`: box-and-whiskers plots ("boxplots")
- `histogram`: histograms
- `stripplot`: like a boxplot but with actual points
- `dotplot`: plot dots on "violin strings"
- `splom`: scatterplot matrix; like pairs in base plotting system
- `levelplot`, `contourplot`: for plotting "image" data
```{r}
xyplot(y ~ x | f * g, data)
xyplot(Life.Exp ~ Income | region, data = state, layout = c(4,1))
xyplot(price~carat|color*cut,data=diamonds,strip = FALSE, pch=20, xlab = myxlab,ylab = myylab,main=mymain)
```
colors defining the columns of the plot; `strip` labels each panel
#### 3.2.2.2 Panel Functions
```{r}
xyplot(y ~ x | f, panel = function(x, y, ...) {
       panel.xyplot(x, y, ...)  ## First call the default panel function for 'xyplot'
       panel.abline(h = median(y), lty = 2)  ## Add a horizontal line at the median
       panel.lmline(x, y, col = 2)  ## Overlay a simple linear regression line
})
```
### 3.2.3 {ggplot2}
#### Axis Limits
`ylim()` subsets the data, to include the values that are between minus 3 and 3. The outlier is not included in this data set and so you won't see that data point in this plot. 
```{r}
g + geom_line() + ylim(-3, 3)
```
`coord_cartesian()` the outlier is included in the dataset.
```{r}
g + geom_line() + coord_cartesian(ylim = c(-3, 3))
```
### 3.2.4 {RColorBrewer}
contains interesting and useful color palettes, of which there are 3 types, sequential, divergent, and qualitative.
```{r}
cols <- brewer.pal(3,"BuGn")
```
- `"BuGn"`: palette name
- `3`: how many different colors we want
The colorRamp and colorRampPalette functions can be used in conjunction with color palettes to connect data to colors.
```{r}
pal <- colorRampPalette(cols)
image(volcano,col=pal(20))
```
## 3.3 Clustering
### 3.3.1 Hierarchical clustering
#### {base}
##### Make distance matrix
```{r}
dataFrame <- data.frame(x=x,y=y)
distxy <- dist(dataFrame)
```
- `method=`: "euclidean" (default), "manhattan", etc.
##### The dendrogram
```{r}
hc <- hclust(distxy)
plot(hc)
plot(as.dendrogram(hc))
```
- `as.dengrogram()`: The essentials are the same, but the labels are missing and the leaves (original points) are all printed at the same level.
### 3.3.2 K-means clustering
#### {base}
```{r}
dataFrame <- data.frame(x,y)
kmeansObj <- kmeans(dataFrame,centers=3)
```
Plot
```{r}
plot(x,y,col=kmeansObj$cluster,pch=19,cex=2)
points(kmeansObj$centers,col=1:3,pch=3,cex=3,lwd=3)
```
### 3.3.3 Heatmaps
[a very nice concise tutorial on creating heatmaps in R](http://sebastianraschka.com/Articles/heatmaps_in_r.html#clustering)
#### {base}
##### Hierarchical clustering
```{r}
dataMatrix <- as.matrix(dataFrame)[sample(1:12),]
heatmap(dataMatrix,col=cm.colors(25))
```
##### K-means clustering
```{r}
par(mfrow=c(1,2), mar = c(2, 4, 0.1, 0.1))
image(t(dataMatrix)[,nrow(dataMatrix):1],yaxt="n")
image(t(dataMatrix)[,order(kmeansObj$cluster)],yaxt="n")
```
### 3.3.4 Further resources
- [Rafa's Distances and Clustering Video](http://www.youtube.com/watch?v=wQhVWUcXM0A)
- [Elements of statistical learning](https://web.stanford.edu/~hastie/ElemStatLearn/)
- [Determining the number of clusters](http://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set)
## 3.4 Principal Component Analysis (PCA)
### {base}
PCA of a scaled matrix yields the V matrix (right singular vectors) of the same scaled matrix.
```{r}
svd(scale(mat))
prcomp(scale(mat))
```
- `prcomp()` returns:
  - `x`: contains the principal components (PCs) for drawing a graph.
  - `sdev`: how much variation in the origin data each PC accounts for
  - `rotation`: loading scores
```{r}
# plot pc1 and pc2
plot(pca$x[,1], pca$x[,2])
 
# make a scree plot
pca.var <- pca$sdev^2
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)

barplot(pca.var.per, main="Scree Plot", xlab="Principal Component", ylab="Percent Variation")
```
- `svd()` returns:
  - `v`: the `rotation` that `prcomp()` returns, a matrix of loading scores
  - `u`: similar to the `x` that `prcomp()` returns. In other words, sum(`rotation` * original data), but compressed to the unit vector. You can spread it out by multiplying by `d`
  - `d`: similar to the `sdev` that `prcomp()` returns (and thus related to the eigen values), but not scaled by sample size in an unbiased way (ie. 1/(n-1)). For `prcomp()`, `sdev` = sqrt(var) = sqrt(ss(fit)/(n-1))  
  
The first column of V is the first principal component of the matrix.
```{r}
hh <- hclust(dist(dataMatrix))
dataMatrixOrdered <- dataMatrix[hh$order,]

svd1 <- svd(scale(dataMatrixOrdered))
pca1 <- prcomp(dataMatrixOrdered,scale=TRUE)
plot(pca1$rotation[,1],svd1$v[,1],pch=19,xlab="Principal Component 1",ylab="Right Singular Vector 1")
abline(c(0,1))
```
#### Components of the SVD - *u* and *v*
- The first column of the U matrix (LEFT singular vector) associated with the scaled data matrix is associated with the ROW means of the clustered data.
- The first column of the V matrix (RIGHT singular vector) associated with the scaled data matrix is associated with the COLUMN means of the clustered data.
- the other columns of U or V don't show this pattern as clearly as this first one does.
```{r}
svd1 <- svd(scale(dataMatrixOrdered))
plot(svd1$u[,1],40:1,xlab="Row",ylab="First left singular vector",pch=19)
plot(svd1$v[,1],xlab="Column",ylab="First right singular vector",pch=19)
```
#### Components of the SVD - Variance explained
```{r}
plot(svd1$d,xlab="Column",ylab="Singular value",pch=19)
plot(svd1$d^2/sum(svd1$d^2),xlab="Column",ylab="Prop. of variance explained",pch=19)
```
### Impute missing data
#### {impute}
- `impute.knn()`: uses the k nearest neighbors to calculate a value to use in place of the missing data
```{r}
dataMatrix2 <- dataMatrixOrdered
dataMatrix2[sample(1:100,size=40,replace=FALSE)] <- NA
dataMatrix2 <- impute.knn(dataMatrix2)$data
svd2 <- svd(scale(dataMatrix2))
plot(svd2$v[,1],pch=19)
```
### Furthur resources
- [A Tutorial on Principal Component Analysis](https://arxiv.org/pdf/1404.1100.pdf)
## 3.5 Further resources
- [R Graph Gallery](https://www.r-graph-gallery.com/)
- [R Bloggers](https://www.r-bloggers.com/)
# 4. Step 4 Statistical Inference
## 4.1 Conditional Probability
- P(A|B) = P(A & B)/ P(B). P(A|B) is the probability that BOTH A and B occur divided by the probability that B occurs.
- Bayes' Rule:
  - P(B|A) = P(B&A)/P(A) = P(A|B) * P(B)/P(A)
  - P(A) = P(A|B) \* P(B) + P(A|~B) \* P(~B)
  - P(B|A) = P(A|B) \* P(B) / ( P(A|B) \* P(B) + P(A|~B) \* P(~B) )

### 4.1.1 Diagnostic tests
- **Sensitivity**: the probability that the test is positive given that the subject actually has the disease, P(+|D)
- **Specificity**: the probability that the test is negative given that the subject does not have the disease, P(-|~D)
- **Positive predictive value**: the probability that the subject has the disease given that the test is positive, P(D|+)
- **Negative predictive value**: the probability that the subject does not have the disease given that the test is negative, P(~D|-)
- **Prevalence of the disease**: the marginal probability of disease, P(D)
- **Diagnostic likelihood ratio of a positive test (DLR+)**: P(+|D)/P(+|~D) or sensitivity/(1-specificity)
- **Diagnostic likelihood ratio of a negative test (DLR-)**: P(-|D)/P(-|~D) or (1-sensitivity)/specificity
 - post-test odds of D = DLR * pre-test odds of D
 - hypothesis of disease is supported (DLR+/-) times that of the hypothesis of no disease given the (positive/negative) test result

## 4.2 Variances
- The sample variance estimates the population variance
- The distribution of the sample variance is centered at what its estimating. It gets more concentrated around the population variance with larger sample sizes
- The variance of the sample mean is the population variance divided by n. The square root is the standard error.
## 4.3 Distributions
### 4.3.1 Binomial distrubtion
Suppose a friend has 8 children, 7 of which are girls and none are twins. If each gender has an independent 50% probability for each birth, what's the probability of getting 7 or more girls out of 8 births?
```{r}
pbinom(6, size = 8, prob = .5, lower.tail = FALSE)
```
### 4.3.2 Normal distribution
- If X~N(mu,sigma^2), then Z=(X-mu)/sigma is standard normal
- If Z is standard normal, then X=mu+sigma*Z~N(mu,sigma^2)  
  
What is the 95th percentile of a N(mu,sigma^2) distribution? (The point x0 so that 
P(X<=x0)=.95)
```{r}
qnorm(.95, mean = mu, sd = sd)
```
or
```{r}
x0=mu+sigma*z0
```
where z0 is the appropriate standard normal quantile (in this case 1.645)  
  
Assume that the number of daily ad clicks for a company is (approximately) normally distributed with a mean of 1020 and a standard deviation of 50. What's the probability of getting more than 1,160 clicks in a day?  
It's not very likely, 1,160 is `r (1160-1020)/50` standard deviations from the mean.
```{r}
pnorm(1160, mean = 1020, sd = 50, lower.tail = FALSE)
pnorm(2.8, lower.tail = FALSE)
```
Assume that the number of daily ad clicks for a company is (approximately) normally distributed with a mean of 1020 and a standard deviation of 50. What number of daily ad clicks would represent the one where 75% of days have fewer clicks (assuming days are independent and identically distributed)?
```{r}
qnorm(0.75, mean = 1020, sd = 50)
```
### 4.3.3 Poisson distribution
- Modeling count data
- Modeling event-time or survival data
- Modeling contingency tables
- Approximating binomials when n is large and p is small 
  
The number of people that show up at a bus stop is Poisson with a mean of 2.5 per hour. If watching the bus stop for 4 hours, what is the probability that 3 or fewer people show up for the whole time?
```{r}
ppois(3, lambda = 2.5 * 4)
```
### 4.3.4 t/student's distribution
- The reason for the t distribution is as follows. If we take x bar, subtract off the mean, and divide it by the estimated standard error for iid Gaussian data, it in fact is not Gaussian distributed. If we replaced s by sigma, it would be exactly standard normal. However, when we replace sigma by s, it no longer has a distribution as that of a standard normal. Instead, it has a t distribution.  
- Assumes that the underlying data are iid Gaussian with the result that (xbar-mu)/(S/sqrt(n)) follows Gosset's t distribution with n-1 degrees of freedom.
## 4.4 Asymptotics
- Law of large numbers (LLN):
  - averages of iid samples converge to the population means that they are estimating
  - the sample mean, sample variance and sample standard deviation of iid random variables are consistent 
    - An estimator is consistent if it converges to what you want to estimate. Typically, good estimators are consistent.
- Central Limit Theorem (CLT):
  - the distribution of averages of iid variables (properly normalized) becomes that of a standard normal as the sample size increases
  - properly normalized:
    - (Estimate-Mean of estimate)/Std. Err. of estimate
    - (sample mean-mean of sample mean)/(sigma/sqrt(n))
  - X(Estimate/sample mean) is approximately N(mu,sigma^2/n)

### 4.4.1 Confidence intervals
Quick CI for proportions
```{r}
phat+c(-1,1)/sqrt(n)
```
Your campaign advisor told you that in a random sample of 100 likely voters, 56 intent to vote for you. Can you relax? Do you have this race in the bag?  
Quick CI
```{r}
.56+c(-1,1)/sqrt(100)
```
#### 4.4.1.1 Binomial interval 
##### Wald intervel (CLT)
```{r}
.56 + c(-1, 1) * qnorm(.975) * sqrt(.56 * .44 / 100)
```
##### Exact intervel: guarantees 95% or higher coverage
Do not rely on the CLT (regardless of the size N). A nice compliment to large sample procedures. They tend to have wider intervals. 
```{r}
binom.test(56, 100)$conf.int
```
##### Agresti/Coull interval
A quick fix for small sample size binomial calculations: Add two successes and failures phat=(X+2)/(n+4). (It's a little conservative)
```{r}
phat <- (rbinom(1, prob = .5, size = 20)+2)/(n+4)
phats + c(-1,1) * qnorm(.975) * sqrt(phats * (1 - phats) / n)
```
#### 4.4.1.2 Poisson interval
A nuclear pump failed 5 times out of 94.32 days, give a 95% confidence interval for the failure rate per day?
##### Wald intervel (CLT)
```{r}
x <- 5
t <- 94.32
lambda <- x/t
round(lambda + c(-1, 1) * qnorm(0.975) * sqrt(lambda/t), 3)
```
##### Exact intervel
```{r}
poisson.test(x, T = 94.32)$conf
```
#### 4.4.1.3 T confidence intervals
- The t interval technically assumes that the data are iid normal, though it is robust to this assumption
- It works well whenever the distribution of the data is roughly symmetric and mound shaped
- Paired observations are often analyzed using the t interval by taking differences
- For large degrees of freedom, t quantiles become the same as standard normal quantiles; therefore this interval converges to the same interval as the CLT yielded
- For skewed distributions, the spirit of the t interval assumptions are violated 
  - Also, for skewed distributions, it doesn't make a lot of sense to center the interval at the mean
  - In this case, consider taking logs or using a different summary like the median
- For highly discrete data, like binary, other intervals are available

##### Paired groups
`data(sleep)` shows the increase in hours for 10 patients on two soporific drugs.  
Manually construct the t CI:
```{r}
g1 <- sleep$extra[1:10]
g2 <- sleep$extra[11:20]
difference <- g2 - g1
mn <- mean(difference)
s <- sd(difference)
n <- 10

mn + c(-1, 1) * qt(0.975, n - 1) * s/sqrt(n)
```
R function:
```{r}
t.test(difference)
t.test(g2, g1, paired = TRUE)
t.test(extra ~ relevel(group, 2), paired = TRUE, data = sleep)
```
##### Independent groups
assume the sleep data is not paired  
Manually construct the t CI:
```{r}
n1 <- length(g1); n2 <- length(g2)
sp <- sqrt( ((n1 - 1) * sd(x1)^2 + (n2-1) * sd(x2)^2) / (n1 + n2-2))
md <- mean(g2) - mean(g1)
semd <- sp * sqrt(1 / n1 + 1/n2)

md + c(-1, 1) * qt(.975, n1 + n2 - 2) * semd
```
R function:
```{r}
t.test(g2, g1, paired = FALSE, var.equal = TRUE)$conf
```
## 4.5 Hypothesis testing
Truth | Decide | Result |
---|---|---|
H0 | H0 | Correctly accept null |
H0 | Ha | Type I error |
Ha | Ha | Correctly reject null |
Ha | H0 | Type II error |
### 4.5.1 t test
Suppose that in a sample of 16 overweight subjects with other risk factors for sleep disordered breathing at a sleep clinic, the mean RDI was 32 events/hour with a standard deviation of 10 events/hour.  
  
Manually construct the t test:  
- The statistic (xbar-30)/(s/sqrt(n)) follows a T distribution with n-1 df under H0
- One-sided test: H0: mu=30. Ha: mu>30
```{r}
(32-30)/(10/sqrt(16)) < qt(.95, 15)
```
so fail to reject Ho  
  
- Two-sided t test: H0: mu=30. Ha: mu!=30
```{r}
(32-30)/(10/sqrt(16)) < qt(.975, 15)
```
If you fail to reject the one-sided test, you know that you will fail to reject the two-sided
  
R function:
```{r}
t.test(father.son$sheight - father.son$fheight)
```

```{r}
t.test(gain ~ Diet, paired = FALSE, 
       var.equal = TRUE, data = wideCW14)
```
### 4.5.2 Exact binomial test
```{r}
pbinom( 6, size = 8, p = .5, lower.tail = FALSE) < .05
```
so we'll reject H0 if we see at least 7 girls in 8 children
## 4.6 P value
The probability under the null hypothesis of obtaining evidence as extreme or more extreme than that obtained.  
  
Suppos that you get a T statistic of 2.5 for 15 df testing H0: mu=mu0, Ha: mu>mu0. What's the probability of getting a T statistic as large as 2.5?
```{r}
pt(2.5, 15, lower.tail = FALSE)
```

```{r}
pbinom(6, size = 8, prob = .5, lower.tail = FALSE)
```
Poisson example: Suppose that a hospital has an infection rate of 10 infections per 100 person/days at risk (rate of 0.1) during the last monitoring period. H0: lambda=0.05, Ha: lambda>0.05.
```{r}
ppois(9, 5, lower.tail = FALSE)
```
## 4.7 Power
- The probability of rejecting the null hypothesis when it is false. It comes more into play for null results, than it does for non null results. And the way the power gets used most often, as at the time of designing the study. You want to design the study, so that you have a reasonable chance of detecting the alternative hypothesis, if the alternative hypothesis is true.  
- Power=1-beta(type 2 error/fail to reject the null hypothesis when it's false)  
  
Ho: xbar~N(mu0,sigma^2/n)
Ha: xbar~N(mua,sigma^2/n)
  
Manually construct power
```{r}
alpha = 0.05
mu0 = 30; mua = 32; sigma = 4; n = 16
z = qnorm(1 - alpha)
power <- pnorm(mu0 + z * sigma / sqrt(n), mean = mua, sd = sigma / sqrt(n), 
      lower.tail = FALSE)
```
There's a 64% probability of detecting a mean as large as 32 or larger if we conduct this experiment. 
  
- (mua-mu0)/sigma: effect size, the difference in the means in standard deviation units.  
- power only depends on effectSize * sqrt(n)
  
R function  
`power.t.test()`: Omit one of the arguments and it solves for it 
```{r}
power.t.test(n = 16, delta = 2, sd=4, type = "one.sample",  alt = "one.sided")$power
```
- `delta=` the difference in the means
```{r}
power.t.test(power = .8, delta = 2, sd=4, type = "one.sample",  alt = "one.sided")$n
```
So you need a sample of size 27, to have a power of 80% to detect an effect size as large as 0.5.  
## 4.8 Multiple testing and correction
### 4.8.1 Type of errors
claim/truth   | beta=0 | beta!=0 | Hypotheses |
--------------|--------|---------|------------|
Claim beta=0  | U | T | m-R |
Claim beta!=0 | V | S | R |
Claims        | m0| m-m0 | m |  

**Type I error or false positive (V)** Say that the parameter does not equal zero when it does.  
**Type II error or false negative (T)** Say that the parameter equals zero when it doesn't.
### 4.8.2 Error rates
- **False positive rate**: The rate at which false results (beta=0) are called significant: E(V/m0)
- **Family wise error rate (FWER)**: The probability of at least one false positive P(V>=1)
- **False discovery rate (FDR)**: The rate at which claims of significance are false: E(V/R)
### 4.8.3 Controll the error rates
#### Controll the false positive rate
If P-values are correctly calculated, calling all P<alpha significant will control the false positive rate at level alpha on average.
#### Controll the family-wise error rate (FWER)
[Bonferroni correction](http://en.wikipedia.org/wiki/Bonferroni_correction)  
- Suppose you do m tests, You want to control FWER at level alpha so Pr(V>=1)<alpha
- Calculate P-values normally
- Set alpha_fwer=alpha/m
- Call all P-values less than alpha_fwer significant 
  
**Pros**: Easy to calculate, conservative  
**Cons**: May be very conservative  
#### Controlling the false discovery rate (FDR)
This is the most popular correction when performing lots of tests say in genomics, imaging, astronomy, or other signal-processing disciplines.  
[Benjamini-Hochberg procedure](https://en.wikipedia.org/wiki/False_discovery_rate)  
- Suppose you do m tests, You want to control FDR at level alpha so E(V/R)<alpha
- Calculate P-values normally
- Order the P-values from smallest to largest P1,...,Pm
- Call any Pi<=alpha*i/m significant  
  
**Pros**: Still pretty easy to calculate, less conservative (maybe much less)  
**Cons**: Allows for more false positives, may behave strangely under dependence  
#### Adjusted P-values
They are not p-values anymore. But they can be used directly without adjusting alpha.
```{r}
# Controls FWER
sum(p.adjust(pValues, method = "bonferroni") < 0.05)
```

```{r}
# Controls FDR
sum(p.adjust(pValues, method = "BH") < 0.05)
```
### 4.8.4 Notes and resources
- If there is strong dependence between tests there may be problems 
  - Consider method="BY"
- [Multiple testing procedures with applications to genomics](http://www.amazon.com/Multiple-Procedures-Applications-Genomics-Statistics/dp/0387493166/ref=sr_1_2/102-3292576-129059?ie=UTF8&s=books&qid=1187394873&sr=1-2)
- [Statistical significance for genome-wide studies](http://www.pnas.org/content/100/16/9440.full)
- [Introduction to multiple testing](http://ies.ed.gov/ncee/pubs/20084018/app_b.asp)

## 4.9 Resampleing
### 4.9.1 Bootstrap
- Simulating complete data sets from the observed data with replacement
- Calculate the statistic for each simulated data set
- Use the simulated statistics to either define a confidence interval or take the standard deviation to calculate a standard error
```{r}
B <- 10000
resamples <- matrix(sample(x,
                           n * B,
                           replace = TRUE),
                    B, n)
medians <- apply(resamples, 1, median)
sd(medians)
quantile(medians, c(.025, .975))
```

```{r}
g = ggplot(data.frame(x = resampledMedians), aes(x = x)) 
g = g + geom_density(size = 2, fill = "red")
#g = g + geom_histogram(alpha = .20, binwidth=.3, colour = "black", fill = "blue", aes(y = ..density..)) 
g = g + geom_vline(xintercept = median(x), size = 2)
g
```
#### Notes
- Better percentile bootstrap confidence intervals correct for bias
  - bias-corrected and accelerated interval (BCA interval) provided in `{bootstrap}` package in R.
- "An Introduction to the Bootstrap" by Efron and Tibshirani

### 4.9.2 Permutation tests
- Consider the null hypothesis that the distribution of the observations from each group is the same
- Then, the group labels are irrelevant
- Consider a data frome with count and spray. Permute the spray (group) labels
- Recalculate the statistic 
  - Mean difference in counts
  - Geometric means
  - T statistic
- Calculate the percentage of simulations where the simulated statistic was more extreme (toward the alternative) than the observed
```{r}
subdata <- InsectSprays[InsectSprays$spray %in% c("B", "C"),]
y <- subdata$count
group <- as.character(subdata$spray)
testStat <- function(w, g) mean(w[g == "B"]) - mean(w[g == "C"])
observedStat <- testStat(y, group)
permutations <- sapply(1 : 10000, function(i) testStat(y, sample(group)))
observedStat
mean(permutations > observedStat)
```

```{r}
g = ggplot(data.frame(permutations = permutations),
           aes(permutations))
g = g + geom_histogram(fill = "lightblue", color = "black", binwidth = 1)
g = g + geom_vline(xintercept = observedStat, size = 2)
g
```
#### Variations on permutation testing
Data type | Statistic | Test name 
---|---|---|
Ranks | rank sum | rank sum test
Binary | hypergeometric prob | Fisher's exact test
Raw data | | ordinary permutation test  
# 5. Step 5 Regression Models
## 5.1 Linear least squares
### Background
- **Centering**: subtract the mean from data points: xi~ = xi - xbar. mean(xi~)=0
- **Scaling**: divide the data points by empirical standard deviation. sd(xi/s)=1
- **Normalizing**: centering then scaling. zi=(xi-xbar)/s. zi~N(0,1)
### Calculation
```{r}
fit3 <- lm(price ~ I(carat * 10), data = diamond)
```
Plot the fitted regression line
```{r}
abline(fit, lwd = 2)
```
### Prediction
```{r}
predict(fit, newdata = data.frame(carat = newx))
```
## 5.2 Residuals
Model Yi=beta0+beta1*Xi+ei where ei~N(0,sigma^2).
```{r}
e <- resid(fit)
```
### Residual plot
```{r}
g = ggplot(data.frame(x = x, y = resid(lm(y ~ x))), 
           aes(x = x, y = y))
g = g + geom_hline(yintercept = 0, size = 2); 
g = g + geom_point(size = 7, colour = "black", alpha = 0.4)
g = g + geom_point(size = 5, colour = "red", alpha = 0.4)
g = g + xlab("X") + ylab("Residual")
g
```
Minor tweak
```{r}
diamond$e <- resid(lm(price ~ carat, data = diamond))
g = ggplot(diamond, aes(x = carat, y = e))
g = g + xlab("Mass (carats)")
g = g + ylab("Residual price (SIN $)")
g = g + geom_hline(yintercept = 0, size = 2)
g = g + geom_point(size = 7, colour = "black", alpha=0.5)
g = g + geom_point(size = 5, colour = "blue", alpha=0.2)
g
```
Plot show the variation explained by the regressor
```{r}
e = c(resid(lm(price ~ 1, data = diamond)),
      resid(lm(price ~ carat, data = diamond)))
fit = factor(c(rep("Itc", nrow(diamond)),
               rep("Itc, slope", nrow(diamond))))
g = ggplot(data.frame(e = e, fit = fit), aes(y = e, x = fit, fill = fit))
g = g + geom_dotplot(binaxis = "y", size = 2, stackdir = "center", binwidth = 20)
g = g + xlab("Fitting approach")
g = g + ylab("Residual price")
g
```
### Residual variance
Manually construct the sigma (estimate)
```{r}
sigma <- sqrt(sum(resid(fit)^2) / (n - 2))
```
R function
```{r}
summary(fit)$sigma
```
## 5.3 Inference in regression
under iid Gaussian errors (betajhat-betaj)/sigmahat follows a t distribution with n-2 degrees of freedom and a normal distribution for large n.  
  
Manually contruct the CI
```{r}
y <- diamond$price; x <- diamond$carat; n <- length(y)
beta1 <- cor(y, x) * sd(y) / sd(x)
beta0 <- mean(y) - beta1 * mean(x)
e <- y - beta0 - beta1 * x
sigma <- sqrt(sum(e^2) / (n-2)) 
ssx <- sum((x - mean(x))^2)
seBeta0 <- (1 / n + mean(x) ^ 2 / ssx) ^ .5 * sigma 
seBeta1 <- sigma / sqrt(ssx)
tBeta0 <- beta0 / seBeta0; tBeta1 <- beta1 / seBeta1
pBeta0 <- 2 * pt(abs(tBeta0), df = n - 2, lower.tail = FALSE)
pBeta1 <- 2 * pt(abs(tBeta1), df = n - 2, lower.tail = FALSE)
coefTable <- rbind(c(beta0, seBeta0, tBeta0, pBeta0), c(beta1, seBeta1, tBeta1, pBeta1))
colnames(coefTable) <- c("Estimate", "Std. Error", "t value", "P(>|t|)")
rownames(coefTable) <- c("(Intercept)", "x")
coefTable
```
Semi-manually contruct the CI
```{r}
sumCoef <- summary(fit)$coefficients
sumCoef[1,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[1, 2]
(sumCoef[2,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[2, 2]) / 10
```
R function
```{r}
summary(fit)$coefficients
confint(fit)
```
### Prediction interval
```{r}
newx = data.frame(x = seq(min(x), max(x), length = 100))
p1 = data.frame(predict(fit, newdata= newx,interval = ("confidence")))
p2 = data.frame(predict(fit, newdata = newx,interval = ("prediction")))
p1$interval = "confidence"
p2$interval = "prediction"
p1$x = newx$x
p2$x = newx$x
dat = rbind(p1, p2)
names(dat)[1] = "y"
```
- `interval = ("confidence")`: regression line interval
- `interval = ("prediction")`: y prediction interval, will always be larger than line interval  
  
Plot the intervals
```{r}
g = ggplot(dat, aes(x = x, y = y))
g = g + geom_ribbon(aes(ymin = lwr, ymax = upr, fill = interval), alpha = 0.2) 
g = g + geom_line()
g = g + geom_point(data = data.frame(x = x, y=y), aes(x = x, y = y), size = 4)
g
```
## 5.4 Multivariable Regression
### 5.4.1 EDA
scatterplot matrix/correlation
```{r}
library(GGally)
g = ggpairs(swiss, lower = list(continuous = "smooth"),params = c(method = "loess"))
g
```
violin plot
```{r}
g = ggplot(data = InsectSprays, aes(y = count, x = spray, fill  = spray))
g = g + geom_violin(colour = "black", size = 2)
g = g + xlab("Type of spray") + ylab("Insect count")
g
```
### 5.4.2 Modelling
```{r}
summary(lm(count ~ spray, data = InsectSprays))
```
Choose another reference level
```{r}
spray2 <- relevel(InsectSprays$spray, "C")
summary(lm(count ~ spray2, data = InsectSprays))
```
Add a interaction term to allow for different slopes and intercepts
```{r}
summary(lm(Fertility ~ Agriculture * factor(CatholicBin), data = swiss))$coef
```
plot these two lines
```{r}
fit = lm(Fertility ~ Agriculture * factor(CatholicBin), data = swiss)
g1 = g
g1 = g1 + geom_abline(intercept = coef(fit)[1], slope = coef(fit)[2], size = 2)
g1 = g1 + geom_abline(intercept = coef(fit)[1] + coef(fit)[3], 
                          slope = coef(fit)[2] + coef(fit)[4], size = 2)
g1
```
### 5.4.3 Residuals in multivariate regression
Our estimate of residual variation is sigmahat^2 = sum(ei^2)/(n-p), so that E(sigmahat^2) = sigma^2. p is number of coefficients including intercept.
#### Residual diagnostics plots
Can be used to measure model fit and detect outliers
```{r}
fit <- lm(Fertility ~ . , data = swiss); plot(fit)
```
#### Influence and leverage
* Do `?influence.measures` to see the full suite of influence measures in stats. The measures include
  * `rstandard` - standardized residuals, residuals divided by their standard deviations)
  * `rstudent` - standardized residuals, residuals divided by their standard deviations, where the ith data point was deleted in the calculation of the standard deviation for the residual to follow a t distribution
  * `hatvalues` - measures of leverage
    *  tremendously useful in finding data entry errors. If you see some data points that have extremely large hatvalues, not only would they potentially impact your model, but you should look at that row to ascertain whether there was a data entry error.
  * `dffits` - change in the predicted response when the ith point is deleted in fitting the model.
  * `dfbetas` - change in individual coefficients when the ith point is deleted in fitting the model.
  * `cooks.distance` - overall change in the coefficients when the ith point is deleted.
  * `resid` - returns the ordinary residuals
  * `resid(fit) / (1 - hatvalues(fit))` where `fit` is the linear model fit returns the PRESS residuals, i.e. the leave one out cross validation residuals - the difference in the response and the predicted response at data point i, where it was not included in the model fitting.

### 5.4.4 Choose the right model (variable inclusion and exclusion)
- Have to bring in some of the specific subject matter, or clinical scientific subject matter expertise into model building exercise.  
- Sometimes the treatment and the other regressor are highly related, but merely because they're related things, that it isn't interesting to adjust for x. So the fact that it causes the treatment effect to go away isn't meaningful. If I have systolic blood pressure in a model, and I put diastolic blood pressure in a model as well, that of course, makes the first one go away. But that's not interesting. I know those two blood pressure measurements are highly correlated. Why should I have them both in the model?  
- omitting a variable that we should have included generally results in bias, unless their regressors are uncorrelated with the omitted ones. 
  - This is why we randomize treatments, it attempts to uncorrelate our treatment indicator with variables that we don't have to put in the model.
- introducing unnecessary variables into regression model inflates the standard errors. Especially if we include variables that are unnecessary that are correlated with our variable of interest. 
- Good design can often eliminate the need for complex model searches at analyses; though often control over the design is limited.
- If the models of interest are nested and without lots of parameters differentiating them, it's fairly uncontroversial to use nested likelihood ratio tests.

#### Variance inflation factors
The variance inflation factor (VIF) is the increase in the variance for the ith regressor compared to the ideal setting where it is uncorrelated to the other regressors. 
```{r}
library(car)
fit <- lm(Fertility ~ . , data = swiss)
vif(fit)
sqrt(vif(fit))
```
#### Residual variance: sigma^2
- Assuming that the model is linear with additive iid errors (with finite variance), we can mathematically describe the impact of omitting necessary variables or including unnecessary ones. 
  - If we underfit the model, the variance estimate is biased.
  - If we correctly or overfit the model, including all necessary covariates and/or unnecessary covariates, the variance estimate is unbiased. 
    - However, the variance of the variance is larger if we include unnecessary variables.

#### Likelihood ratio tests (nested model testing)
```{r}
fit1 <- lm(Fertility ~ Agriculture, data = swiss)
fit3 <- update(fit, Fertility ~ Agriculture + Examination + Education)
fit5 <- update(fit, Fertility ~ Agriculture + Examination + Education + Catholic + Infant.Mortality)
anova(fit1, fit3, fit5)
```
## 5.5 Generalized linear model
In GLMs, we don't log the outcome itself, or we don't take a transformation of the outcome itself, we take transformation of the mean of the outcome.
### 5.5.1 Logistic regression
- Assume that Yi~Bernoulli(mui) so that E(Yi) = mui where 0<=mui<=1.
- Linear predictor etai = sum(Xik*betak)
- Link function g(mu)=eta=log(mu/(1-mu)), g is the (natural) log odds, referred to as the logit.
- Then we can invert the logit function as mui=exp(etai)/(1+exp(etai)), and 1-mui=1/(1+exp(etai))
- Pr(RWi|RSi,bo,b1)=exp(b0+b1\*RSi)/(1+exp(b0+b1*RSi))
- log(Pr(RWi|RSi,bo,b1)/(1-Pr(RWi|RSi,bo,b1)))=b0+b1*RSi  
  
Calculate the regression
```{r}
logRegRavens <- glm(ravensData$ravenWinNum ~ ravensData$ravenScore,family="binomial")
```
plot the fitted value
```{r}
plot(ravensData$ravenScore,logRegRavens$fitted,pch=19,col="blue",xlab="Score",ylab="Prob Ravens Win")
```
Odds ratios and confidence intervals
```{r}
exp(logRegRavens$coeff)
exp(confint(logRegRavens))
```
ANOVA for logistic regression
```{r}
anova(logRegRavens,test="Chisq")
```
It is especially useful if you have a factor variable, because sometimes you want the factor variable included or the factor variable, all levels of the factor variable removed. Then you might want to test whether or not that all levels of them are in total are necessary in the model. Notice that when you do summary and just get the coefficient table out from R, that test each one of the levels of the factor independently and doesn't test them all as a whole. So something like the ANOVA function is useful for putting a factor in and out of the model. 
#### Interpret Logistic Regression
- b0: Log odds of a Ravens win if they score zero points
- b1: Log odds ratio of win probability for each point scored (compared to zero points)
- exp(b1): Odds ratio of win probability for each point scored (compared to zero points)
- [FAQ: How do I interpret odds ratios in logistic regression?](https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/)
- Odds ratio < 0.5 or > 2 commonly a "moderate effect", but is incredibly dependent on the context
#### Further resources
- [Wikipedia on Logistic Regression](http://en.wikipedia.org/wiki/Logistic_regression)
- [Logistic regression and glms in R](http://data.princeton.edu/R/glms.html)
- Brian Caffo's lecture notes on: [Simpson's paradox](http://ocw.jhsph.edu/courses/MethodsInBiostatisticsII/PDFs/lecture23.pdf), [Case-control studies](http://ocw.jhsph.edu/courses/MethodsInBiostatisticsII/PDFs/lecture24.pdf)
- [Open Intro Chapter on Logistic Regression](https://www.openintro.org/stat/videos.php)
 
### 5.5.2 Linear regression on log scale
- log(NHi)=b0+b1*JDi+ei  
- E(log(NHi|JDi,b0,b1))=b0+b1*JDi
  
Calculate the exp coefficients
```{r}
round(exp(coef(lm(I(log(gaData$visits + 1)) ~ gaData$julian))), 5)
```
- There's a problem with logs with you have zero counts, adding a constant works  
#### Interpret the log-scale linear regression (relative)
- exp(E(log(Y))): geometric mean of Y
- exp(beta0): estimated geometric mean hits on day 0
- exp(beta1): estimated relative (percent) increase or decrease in geometric mean hits per day
### 5.5.3 Poisson regression
- Assume that Yi~Poisson(mui) so that E(Yi) = mui where 0<=mui.
- Linear predictor etai = sum(Xik*betak)
- Link function g(mu)=eta=log(mu)
- mui=exp(etai)   
  
- log(E(NHi|JDi,b0,b1))=b0+b1*JDi
- E(NHi|JDi,b0,b1)=exp(b0+b1*JDi)
- avoid problems like taking logs of 0  
  
plot the fitted value
```{r}
plot(gaData$julian,gaData$visits,pch=19,col="darkgrey",xlab="Julian",ylab="Visits")
glm1 <- glm(gaData$visits ~ gaData$julian,family="poisson")
abline(lm1,col="red",lwd=3); lines(gaData$julian,glm1$fitted,col="blue",lwd=3)
```
#### Interpret Poisson Regression
- If JDi is increased by one unit, E(NHi|JDi,b0,b1) is multiplied by exp(b1)
#### Concern related to mean-variance relationship
```{r}
plot(glm1$fitted,glm1$residuals,pch=19,col="grey",ylab="Residuals",xlab="Fitted")
```
Variance has to equal the mean for poisson distribution. In the case they are not equal, use:
- quasi-Poisson model: look at the variance being a constant multiple of the mean, rather than being equal to the mean. 
- sandwich variance estimator, made famous by generalized estimating equations
```{r}
library(sandwich)
confint(glm1)
confint.agnostic(glm1)
```
#### Proportion/Rate
- E(NHSSi|JDi,b0,b1)/NHi=exp(b0+b1*JDi)
- log(E[NHSSi]|JDi,b0,b1)=log(NHi)+b0+b1*JDi
```{r}
glm2 <- glm(gaData$simplystats ~ julian(gaData$date),offset=log(visits+1),
            family="poisson",data=gaData)
```

```{r}
glm(count~x+offset(log(t)),family=poisson)
```
plot
```{r}
plot(julian(gaData$date),glm2$fitted,col="blue",pch=19,xlab="Date",ylab="Fitted Counts")
points(julian(gaData$date),glm1$fitted,col="red",pch=19)
```

```{r}
plot(julian(gaData$date),gaData$simplystats/(gaData$visits+1),col="grey",xlab="Date", ylab="Fitted Rates",pch=19)
lines(julian(gaData$date),glm2$fitted/(gaData$visits+1),col="blue",lwd=3)
```
#### Further resources
- [Log-linear models and multiway tables](http://ww2.coastal.edu/kingw/statistics/R-tutorials/loglin.html)
- [Wikipedia on Poisson regression](http://en.wikipedia.org/wiki/Poisson_regression), [Wikipedia on overdispersion](http://en.wikipedia.org/wiki/Overdispersion)
- [Regression models for count data in R](http://cran.r-project.org/web/packages/pscl/vignettes/countreg.pdf)
- [pscl package](http://cran.r-project.org/web/packages/pscl/index.html): the function `zeroinfl()` fits zero inflated (zero occurs way more often that it should) models. 
### 5.5.4 Assumptions on variances
- For the linear model var(Yi)=sigma^2 is constant.
- For Bernoulli case var(Yi) = mui*(1-mui)
- For Poisson case var(Yi)=mui.
- For data doesn't adhere to the GLM variant structure, it is often relevant to have a more flexible variance model, even if it doesn't correspond to an actual likelihood. These are called 'quasi-likelihood' normal equations. 
## 5.6 Fit functions using linear models
- Yi=beta0+beta1*Xi+sum(xi-xik)+\*gammak+ei
  - (a)+=a if a>0 and 0 otherwise
- The collection of regressors is called a basis
```{r}
n <- 500; x <- seq(0, 4 * pi, length = n); y <- sin(x) + rnorm(n, sd = .3)
knots <- seq(0, 8 * pi, length = 20); 
splineTerms <- sapply(knots, function(knot) (x > knot) * (x - knot))
xMat <- cbind(x, splineTerms)
yhat <- predict(lm(y ~ xMat))
plot(x, y, frame = FALSE, pch = 21, bg = "lightblue", cex = 2)
lines(x, yhat, col = "red", lwd = 2)
```
### Add squared terms
- Yi=beta0+beta1\*Xi+beta2\*Xi^2+sum(xi-xik)+^2*gammak+ei
```{r}
splineTerms <- sapply(knots, function(knot) (x > knot) * (x - knot)^2)
xMat <- cbind(x, x^2, splineTerms)
```
### Model harmonics using the Fourier basis
```{r}
##Chord finder, playing the white keys on a piano from octave c4 - c5
notes4 <- c(261.63, 293.66, 329.63, 349.23, 392.00, 440.00, 493.88, 523.25)
t <- seq(0, 2, by = .001); n <- length(t)
c4 <- sin(2 * pi * notes4[1] * t); e4 <- sin(2 * pi * notes4[3] * t); 
g4 <- sin(2 * pi * notes4[5] * t)
chord <- c4 + e4 + g4 + rnorm(n, 0, 0.3)
x <- sapply(notes4, function(freq) sin(2 * pi * freq * t))
fit <- lm(chord ~ x - 1)
```

```{r}
plot(c(0, 9), c(0, 1.5), xlab = "Note", ylab = "Coef^2", axes = FALSE, frame = TRUE, type = "n")
axis(2)
axis(1, at = 1 : 8, labels = c("c4", "d4", "e4", "f4", "g4", "a4", "b4", "c5"))
for (i in 1 : 8) abline(v = i, lwd = 3, col = grey(.8))
lines(c(0, 1 : 8, 9), c(0, coef(fit)^2, 0), type = "l", lwd = 3, col = "red")
```

```{r}
##(How you would really do it)
a <- fft(chord); plot(Re(a)^2, type = "l")
```
# 6. Step 6 Machine Learning
## 6.1 Resources
- [List of machine learning resources on Quora](http://www.quora.com/Machine-Learning/What-are-some-good-resources-for-learning-about-machine-learning-Why)
- [List of machine learning resources from Science](http://www.sciencemag.org/site/feature/data/compsci/machine_learning.xhtml)
- [Advanced notes from MIT open courseware](http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-867-machine-learning-fall-2006/lecture-notes/)
- [Advanced notes from CMU](http://www.stat.cmu.edu/~cshalizi/350/)
- [Kaggle - machine learning competitions](http://www.kaggle.com/)
## 6.2 Functions
- Some preprocessing (cleaning)
  - preProcess
- Data splitting
  - createDataPartition
  - createResample
  - createTimeSlices
- Training/testing functions
  - train
  - predict
- Model comparison
  - confusionMatrix

## 6.3 Data slicing
```{r}
library(caret); library(kernlab); data(spam)
inTrain <- createDataPartition(y=spam$type,
                              p=0.75, list=FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
dim(training)
```
Cross validation: K-fold
```{r}
folds <- createFolds(y=spam$type,k=10,
                             list=TRUE,returnTrain=TRUE)
sapply(folds,length)
folds[[1]][1:10]
```
- `list=TRUE` return each set of imbecies corresponding to a particular fold as a set of as a list.  
- `returnTrain=` choose either to return training set or test set  
  
Cross validation: Resampling
```{r}
folds <- createResample(y=spam$type,times=10,
                             list=TRUE)
sapply(folds,length)
folds[[1]][1:10]
```
Time Slices
```{r}
tme <- 1:1000
folds <- createTimeSlices(y=tme,initialWindow=20,
                          horizon=10)
names(folds)
folds$train[[1]]
folds$test[[1]]
```
## 6.4 Training options
- `preProcess =`
- `weights =` upweight or downweight certain observations. Particularly useful if you have very unbalanced training set where you have a lot more examples of one type than another.
- `metric =` default metric for factor variable is accuracy that it's trying to maximize. For continuous variables it's the root mean squared error.
- `trControl = trainControl()`
### 6.4.1 Metric
- Continous outcomes:
  * _RMSE_ = Root mean squared error
  * _RSquared_ = R^2 from regression models
- Categorical outcomes:
  * _Accuracy_ = Fraction correct
  * _Kappa_ = A measure of [concordance](http://en.wikipedia.org/wiki/Cohen%27s_kappa)

### 6.4.2 trainControl
- `method =` method to use for resampling the data: bootstrapping or cross validation
- `number =` the number of times to do bootstrapping or cross validation
- `repeats =` times to repeat that whole process if you want to be careful about repeated cross-validation  
- `p =` the size of the training set
- `initialWindow =` for time course data, initial window tells you the size of the number of time points that will be in the training data
- `horizon =` the number of time points that you'll be predicting
- `savePredictions =` return the actual predictions themselves from each of the iterations when it's building the model
- `summaryFunction =` use a different summary function than the default summary if you'd like it to
- `seeds =` set a seed for each resample, useful for parallel fits
- `allowParallel = TRUE` if you're training models on large numbers of samples with a high number of predictors, using parallel processing can be highly useful for improving the computational efficiency of your analysis
#### trainControl resampling
* _method_
  * _boot_ = bootstrapping
  * _boot632_ = bootstrapping with adjustment: reduce some of the bias due to bootstrapping
  * _cv_ = cross validation
  * _repeatedcv_ = repeated cross validation: sub cross validation with different random draws
  * _LOOCV_ = leave one out cross validation
* _number_
  * For boot/cross validation
  * Number of subsamples to take
* _repeats_
  * Number of times to repeate subsampling
  * If big this can _slow things down_

## 6.5 Plot predictors
### {base}
#### Tables
```{r}
t1 <- table(cutWage,training$jobclass)
prop.table(t1, margin = 1)
```
- `margin =` 1 for row, 2 for column  
#### Histogram
```{r}
hist(training$capitalAve,main="",xlab="ave. capital run length")
```
### {caret}
#### Feature plot
```{r}
featurePlot(x=training[,c("age","education","jobclass")],
            y = training$wage,
            plot="pairs")
```
### {ggplot2}
#### Regression line
```{r}
qq <- qplot(age,wage,colour=education,data=training)
qq + geom_smooth(method='lm',formula=y~x) # different line for each factor variable (education)
```
#### Arrange the display of plots
```{r}
p1 <- qplot(cutWage,age, data=training,fill=cutWage,
      geom=c("boxplot"))
p2 <- qplot(cutWage,age, data=training,fill=cutWage,
      geom=c("boxplot","jitter"))
grid.arrange(p1,p2,ncol=2)
```
#### Density plots
```{r}
qplot(wage,colour=education,data=training,geom="density")
```
### {hmisc}
#### cut continuous variable into factor variable
```{r}
cutWage <- cut2(training$wage,g=3)
table(cutWage)
```
## 6.6 Preprocessing
Pre-processing can be more useful often when you're using model based approaches, than when you're using a non-parametric approaches.  
### 6.6.1 Why preprocess?
```{r}
hist(training$capitalAve,main="",xlab="ave. capital run length")
mean(training$capitalAve)
sd(training$capitalAve)
```
### 6.6.2 Standardizing
#### Manually standardizing
```{r}
trainCapAve <- training$capitalAve
trainCapAveS <- (trainCapAve  - mean(trainCapAve))/sd(trainCapAve) 
mean(trainCapAveS)
sd(trainCapAveS)
```
Use the mean and the standard deviation from the training set, to standardize the testing set values.
```{r}
testCapAve <- testing$capitalAve
testCapAveS <- (testCapAve  - mean(trainCapAve))/sd(trainCapAve) 
mean(testCapAveS)
sd(testCapAveS)
```
#### `preProcess()`
```{r}
preObj <- preProcess(training[,-58],method=c("center","scale")) # leave out the outcome variable (the 58th variable)
trainCapAveS <- predict(preObj,training[,-58])$capitalAve
mean(trainCapAveS)
sd(trainCapAveS)
```
`preProcess()` on test set
```{r}
testCapAveS <- predict(preObj,testing[,-58])$capitalAve
mean(testCapAveS)
sd(testCapAveS)
```
#### `preProcess =`
```{r}
modelFit <- train(type ~.,data=training,
                  preProcess=c("center","scale"),method="glm")
```
- `preProcess=c("center","scale")` center and scale all of the predictors, before using those predictors in the prediction model.
#### Box-Cox transforms
Box cox transforms are a set of transformations that take continuous data, and try to make them look like normal data by estimating a specific set of parameters using maximum likelihood. This is a continuous transform and it doesn't take care of values that are repeated. It does take care of a lot of the problems that would occur with using a variable that's highly skewed. 
```{r}
preObj <- preProcess(training[,-58],method=c("BoxCox"))
trainCapAveS <- predict(preObj,training[,-58])$capitalAve
par(mfrow=c(1,2)); hist(trainCapAveS); qqnorm(trainCapAveS)
```
### 6.6.3 Imputing data
```{r}
preObj <- preProcess(training[,-58],method="knnImpute")
capAve <- predict(preObj,training[,-58])$capAve
```
- `method="knnImpute"` K-nearest neighbors computation find the k (say 10) nearest data vectors that look most like data vector with the missing value, and average the values of the variable that's missing and compute them at that position.
Fit a model
#### Check the difference between the imputed data and the true data
```{r}
# Make some values NA
training$capAve <- training$capitalAve
selectNA <- rbinom(dim(training)[1],size=1,prob=0.05)==1
training$capAve[selectNA] <- NA

# Standardize true values
capAveTruth <- training$capitalAve
capAveTruth <- (capAveTruth-mean(capAveTruth))/sd(capAveTruth)

# Checking the difference
quantile(capAve - capAveTruth)
quantile((capAve - capAveTruth)[selectNA])
quantile((capAve - capAveTruth)[!selectNA])
```
### 6.6.4 Covariate creation
#### Level 1: Raw data -> covariates
- Depends heavily on application. Science is key. Google "feature extraction for [data type]"
- The balancing act is summarization vs. information loss
- Examples:
  - Text files: frequency of words, frequency of phrases ([Google ngrams](https://books.google.com/ngrams)), frequency of capital letters.
  - Images: Edges, corners, blobs, ridges ([computer vision feature detection](http://en.wikipedia.org/wiki/Feature_detection_(computer_vision)))
  - Webpages: Number and type of images, position of elements, colors, videos ([A/B Testing](http://en.wikipedia.org/wiki/A/B_testing))
  - People: Height, weight, hair color, sex, country of origin. 
- The more knowledge of the system you have the better the job you will do. 
- When in doubt, err on the side of more features
- Can be automated, but use caution! In some applications (images, voices) automated feature creation is possible/necessary.
  - [deep learning tutorial](http://www.cs.nyu.edu/~yann/talks/lecun-ranzato-icml2013.pdf)

#### Level 2: Tidy covariates -> new covariates
- More necessary for some methods (regression, svms: depend a little bit more on what the distribution of the data are) than for others (classification trees: don't necessarily have as much model-based prediction, don't depend quite so much on the data looking a particular way).
- Should be done only on the training set
- The best approach is through exploratory analysis (plotting/tables)
- New covariates should be added to data frames
##### Common covariates to add: dummy variables
```{r}
table(training$jobclass)
dummies <- dummyVars(wage ~ jobclass,data=training)
head(predict(dummies,newdata=training))
```
##### Removing zero covariates (variables that have very low variance, e.g. only a few unique values)
```{r}
nsv <- nearZeroVar(training,saveMetrics=TRUE)
nsv
```
- `saveMetrics=TRUE` save the metrics so that we can see how it's calculating what the variables are
##### Fit high degree line: Spline basis
use the `gam` method in {caret} which allows smoothing of multiple variables
```{r}
library(splines)
bsBasis <- bs(training$age,df=3) 
bsBasis
```
- See also: `ns()`,`poly()`
```{r}
lm1 <- lm(wage ~ bsBasis,data=training)
plot(training$age,training$wage,pch=19,cex=0.5)
points(training$age,predict(lm1,newdata=training),col="red",pch=19,cex=0.5)
```
Splines on the test set
```{r}
predict(bsBasis,age=testing$age)
```
### 6.6.5 Preprocessing with principal components analysis (PCA)
#### Find correlated predictors
```{r}
M <- abs(cor(training[,-58]))
diag(M) <- 0
which(M > 0.8,arr.ind=T)
```

```{r}
library(corrplot)
p.mat <- cor.mtest(mtcars)$p
corrplot(M, type="lower", order = "hclust", p.mat = p.mat, sig.level = 0.05)
```
#### PCA in R: `prcomp()`
```{r}
smallSpam <- spam[,c(34,32)]
prComp <- prcomp(smallSpam)
plot(prComp$x[,1],prComp$x[,2])
```

```{r}
typeColor <- ((spam$type=="spam")*1 + 1)
prComp <- prcomp(log10(spam[,-58]+1))
plot(prComp$x[,1],prComp$x[,2],col=typeColor,xlab="PC1",ylab="PC2")
```
- log 10 transform+1, to make the data look a little bit more Gaussian (normal looking), because some of the variables are skewed. Often have to do that for principal component analysis to look sensible.  
  
Rotation matrix: how it's summing up the two variable to get each of the principal components.
```{r}
prComp$rotation
```
#### PCA in {caret}
```{r}
preProc <- preProcess(log10(spam[,-58]+1),method="pca",pcaComp=2)
spamPC <- predict(preProc,log10(spam[,-58]+1))
plot(spamPC[,1],spamPC[,2],col=typeColor)
```
- `predict()` if you get a new observation you have to predict what the principle component will look like for that new observation.
```{r}
preProc <- preProcess(log10(training[,-58]+1),method="pca",pcaComp=2)
trainPC <- predict(preProc,log10(training[,-58]+1))
modelFit <- train(y = training$type, x = trainPC, method="glm")
```
test the model with PC
```{r}
testPC <- predict(preProc,log10(testing[,-58]+1))
confusionMatrix(testing$type,predict(modelFit,testPC))
```
##### Alternative
```{r}
modelFit <- train(training$type ~ .,method="glm",preProcess="pca",data=training)
confusionMatrix(testing$type,predict(modelFit,testing))
```
#### Final thoughts on PCA
- Most useful for linear-type models
- Can make it harder to interpret predictors
- Watch out for outliers! 
  - Transform first (with logs/Box Cox)
  - Plot predictors to identify problems
- [Elements of Statistical Learning](http://statweb.stanford.edu/~tibs/ElemStatLearn/)

### 6.6.6 Notes and further reading
- Training and test must be processed in the same way. Anything you do to the training set will create a set of parameters. You must use only those parameters when you apply them to the test set. 
- Test transformations will likely be imperfect, especially if the test/training sets collected at different times
- Careful when transforming factor variables (binary predictors are expected to be not pre-processed)
- [preprocessing with caret](http://topepo.github.io/caret/pre-processing.html)  
## 6.7 Predicting
### 6.7.1 Regression
#### Manually build a model
```{r}
library(caret)
inTrain <- createDataPartition(y=faithful$waiting,
                              p=0.5, list=FALSE)
trainFaith <- faithful[inTrain,]; testFaith <- faithful[-inTrain,]

plot(trainFaith$waiting,trainFaith$eruptions,pch=19,col="blue",xlab="Waiting",ylab="Duration")
lm1 <- lm(eruptions ~ waiting,data=trainFaith)
```
Plot the fitted value against the predictor
```{r}
lines(trainFaith$waiting,lm1$fitted,lwd=3)
```
Predict with new value
```{r}
newdata <- data.frame(waiting=80)
predict(lm1,newdata)
```
test the model fit with test set
```{r}
plot(testFaith$waiting,testFaith$eruptions,pch=19,col="blue",xlab="Waiting",ylab="Duration")
lines(testFaith$waiting,predict(lm1,newdata=testFaith),lwd=3)
```
Get training set/test set errors
```{r}
# Calculate RMSE on training
sqrt(sum((lm1$fitted-trainFaith$eruptions)^2))

# Calculate RMSE on test
sqrt(sum((predict(lm1,newdata=testFaith)-testFaith$eruptions)^2))
```
Prediction intervals
```{r}
pred1 <- predict(lm1,newdata=testFaith,interval="prediction")
ord <- order(testFaith$waiting)
plot(testFaith$waiting,testFaith$eruptions,pch=19,col="blue")
matlines(testFaith$waiting[ord],pred1[ord,],type="l",col=c(1,2,2),lty = c(1,1,1), lwd=3)
```
#### {caret}
```{r}
modFit <- train(eruptions ~ waiting,data=trainFaith,method="lm")
summary(modFit$finalModel)
```

```{r}
modFit<- train(wage ~ age + jobclass + education,
               method = "lm",data=training)
finMod <- modFit$finalModel
print(modFit)
```
##### (Residual) Diagnostics
```{r}
plot(finMod,1,pch=19,cex=0.5,col="#00000010")
```
plot the fitted model versus the residuals then coloring it by different variables to identify potential trends. 
```{r}
qplot(finMod$fitted,finMod$residuals,colour=race,data=training)
```
Residuals shouldn't have any relationship to the order in which the observations appear in your data set. Unless, and this is what's typically you discover when you see a trend like this, or outliers like this at one end of this plot, that there's a relationship with respect to time, or age, or some other continuous variable that the rows are ordered by. It suggests that there's a variable missing from your model. 
```{r}
plot(finMod$residuals,pch=19)
```
Predicted versus truth in test set, you can explore and try to identify trends that you might have missed. So, for example, here we're looking at the year. 
```{r}
pred <- predict(modFit, testing)
qplot(wage,pred,colour=year,data=testing)
```
### 6.7.2 Trees
#### Measures of impurity
- Misclassification Error: 1 - phat_mk(m); k(m)= most common class k (1 - the probability that you're equal to the most common class in that particular leaf)
  - 0 = perfect purity
  - 0.5 = no purity
- Gini index: 1-sum(p_mk^2) (1 - sum of the squared probabilities that you belong to any of the different classes)
  - 0 = perfect purity
  - 0.5 = no purity
- Deviance (natural log)/information gain (log2): -sum(p_mk*log2(p_mk))
  - 0 = perfect purity
  - 1 = no purity
- [http://en.wikipedia.org/wiki/Decision_tree_learning](http://en.wikipedia.org/wiki/Decision_tree_learning)

#### Example
Create training and test sets
```{r}
inTrain <- createDataPartition(y=iris$Species,
                              p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
```
EDA: Iris petal widths/sepal width
```{r}
qplot(Petal.Width,Sepal.Width,colour=Species,data=training)
```
Building model
```{r}
modFit <- train(Species ~ .,method="rpart",data=training)
print(modFit$finalModel)
```
Plot tree
```{r}
plot(modFit$finalModel, uniform=TRUE, 
      main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
```
Prettier plots
```{r}
library(rattle)
fancyRpartPlot(modFit$finalModel)
```
Predict new values
```{r}
predict(modFit,newdata=testing)
```
#### Notes and further resources
* Classification trees are non-linear models
  * They use interactions between variables (if you have a large number of classes for a particular variable that you're predicting with. The models can over-fit a little bit.)
  * Data transformations may be less important (monotone transformations: any transformation that doesn't change the order of the values, but maybe makes them bigger or smaller. Then you will get the same data splits, with the classification or regression trees)
  * Trees can also be used for regression problems (continuous outcome, use RMSE as a measure of impurity)
* Note that there are multiple tree building options
in R both in the caret package - [party](http://cran.r-project.org/web/packages/party/index.html), [rpart](http://cran.r-project.org/web/packages/rpart/index.html) and out of the caret package - [tree](http://cran.r-project.org/web/packages/tree/index.html)
* [Classification and regression trees](http://www.amazon.com/Classification-Regression-Trees-Leo-Breiman/dp/0412048418)

### 6.7.3 Bootstrap aggregating (bagging)
- Basic idea:
  - Resample cases and recalculate predictions
  - Average or majority vote
- Notes:
  - Similar bias
  - Reduced variance
  - More useful for non-linear functions

#### Manually bagging
```{r}
library(ElemStatLearn)
ozone <- ozone[order(ozone$ozone),]
```

```{r}
ll <- matrix(NA,nrow=10,ncol=155)
for(i in 1:10){
  ss <- sample(1:dim(ozone)[1],replace=T)
  ozone0 <- ozone[ss,]; ozone0 <- ozone0[order(ozone0$ozone),]
  loess0 <- loess(temperature ~ ozone,data=ozone0,span=0.2)
  ll[i,] <- predict(loess0,newdata=data.frame(ozone=1:155))
}
```
- `span =` a measure of how smooth that fit will be
```{r}
plot(ozone$ozone,ozone$temperature,pch=19,cex=0.5)
for(i in 1:10){lines(1:155,ll[i,],col="grey",lwd=2)}
lines(1:155,apply(ll,2,mean),col="red",lwd=2)
```
#### {caret}
- in `train()` consider `method` options: `bagEarth`, `treebag`, `bagFDA`
- Alternatively you can bag any model you choose using the `bag()` function  
##### custom bagging
- Take your predictor variable and put it into one data frame
```{r}
predictors = data.frame(ozone=ozone$ozone)
temperature = ozone$temperature
treebag <- bag(predictors, temperature, B = 10,
                bagControl = bagControl(fit = ctreeBag$fit,
                                        predict = ctreeBag$pred,
                                        aggregate = ctreeBag$aggregate))
```
- `B =` the number of replications with the number of sub samples I'd like to take from the data set
- `fit =` the function that's going to be applied to fit the model every time. Could be a call to `train()`
- `predict =` predict new values using a particular model. Could be a call to the `predict()` from a trained model.
- `aggregate =`  the way that we'll put the predictions together, such as mean
- `ctreeBag$fit` takes the predictors data frame and the outcome that we've passed, then uses `ctree()` to train a conditional regression tree on the data set, returns model fit
- `ctreeBag$pred` takes the object from the (`ctree()`) model fit, and a new data set x, and it's going to get a new prediction: Calculates each time the tree response (outcome) from the object and new data; then calculates probability matrix. Returns either the observed levels that it predicts or the predicted response. 
plot
- `ctreeBag$aggregate` get the prediction from every model fits across a large number of observations. Then binds them together into one data matrix with each row being equal to the prediction from one of the model predictions. And then it takes the median at every value. So in other words it takes the median prediction from each of the different model fits across all the bootstrap samples. 
```{r}
plot(ozone$ozone,temperature,col='lightgrey',pch=19)
points(ozone$ozone,predict(treebag$fits[[1]]$fit,predictors),pch=19,col="red")
points(ozone$ozone,predict(treebag,predictors),pch=19,col="blue")
```
#### Further resources
- [Bagging wiki](http://en.wikipedia.org/wiki/Bootstrap_aggregating)
- [Bagging and boosting](http://stat.ethz.ch/education/semesters/FS_2008/CompStat/sk-ch8.pdf)  
### 6.7.4 Random Forests
- Bootstrap samples
- At each split, bootstrap variables
- Grow multiple trees and vote  
  
**Pros**:
- Accuracy  
  
**Cons**:
- Speed
- Interpretability
- Overfitting (use cross validation `rfcv()`)  
#### Example
```{r}
inTrain <- createDataPartition(y=iris$Species,
                              p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
```

```{r}
modFit <- train(Species~ .,data=training,method="rf",prox=TRUE)
modFit
```
- `prox=TRUE` produces a little bit of extra information  
#####  Look at a specific tree
```{r}
getTree(modFit$finalModel,k=2)
```
##### Class "centers"
```{r}
irisP <- classCenter(training[,c(3,4)], training$Species, modFit$finalModel$prox)
irisP <- as.data.frame(irisP); irisP$Species <- rownames(irisP)
p <- qplot(Petal.Width, Petal.Length, col=Species,data=training)
p + geom_point(aes(x=Petal.Width,y=Petal.Length,col=Species),size=5,shape=4,data=irisP)
```
##### Predicting new values
```{r}
pred <- predict(modFit,testing)
table(pred,testing$Species)
```
Plot the error prediction
```{r}
testing$predRight <- pred==testing$Species
qplot(Petal.Width,Petal.Length,colour=predRight,data=testing,main="newdata Predictions")
```
#### Further resources
- [Random forests](http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm)
- [Random forest Wikipedia](http://en.wikipedia.org/wiki/Random_forest)
- [http://www.robots.ox.ac.uk/~az/lectures/ml/lect5.pdf](http://www.robots.ox.ac.uk/~az/lectures/ml/lect5.pdf)
### 6.7.5 Boosting
- Take lots of (possibly) weak predictors
- Weight them and add them up
- Get a stronger predictor  
#### More detail  
- Start with a set of classifiers h1,...,hk
  - Examples: All possible trees, all possible regression models, all possible cutoffs.
- Create a classifier that combines classification functions: f(x)=sgn(sum(alpha_t*h_t(x))).
  - Goal is to minimize error (on training set)
  - Iterative, select one h at each step
  - Calculate weights based on errors
  - Upweight missed classifications and select next h

#### Boosting in R
* Boosting can be used with any subset of classifiers
* One large subclass is [gradient boosting](http://en.wikipedia.org/wiki/Gradient_boosting)
* R has multiple boosting libraries. Differences include the choice of basic classification functions and combination rules.
  * [gbm](http://cran.r-project.org/web/packages/gbm/index.html) - boosting with trees.
  * [mboost](http://cran.r-project.org/web/packages/mboost/index.html) - model based boosting
  * [ada](http://cran.r-project.org/web/packages/ada/index.html) - statistical boosting based on [additive logistic regression](http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.aos/1016218223)
  * [gamBoost](http://cran.r-project.org/web/packages/GAMBoost/index.html) for boosting generalized additive models
* Most of these are available in the caret package 

#### Example
```{r}
library(ISLR)
Wage <- subset(Wage,select=-c(logwage))
inTrain <- createDataPartition(y=Wage$wage,
                              p=0.7, list=FALSE)
training <- Wage[inTrain,]; testing <- Wage[-inTrain,]
```

```{r}
modFit <- train(wage ~ ., method="gbm",data=training,verbose=FALSE)
print(modFit)
```
- `verbose=FALSE`: "gmb" produces a lot of output  
  
Plot
```{r}
qplot(predict(modFit,testing),wage,data=testing)
```
#### Further reading
* A couple of nice tutorials for boosting
  * Freund and Shapire - [http://www.cc.gatech.edu/~thad/6601-gradAI-fall2013/boosting.pdf](http://www.cc.gatech.edu/~thad/6601-gradAI-fall2013/boosting.pdf)
  * Ron Meir- [http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf](http://webee.technion.ac.il/people/rmeir/BoostingTutorial.pdf)
  * [Adaboost on Wikipedia](http://en.wikipedia.org/wiki/AdaBoost)
* Boosting, random forests, and model ensembling are the most common tools that win Kaggle and other prediction contests. 
  * [http://www.netflixprize.com/assets/GrandPrize2009_BPC_BigChaos.pdf](http://www.netflixprize.com/assets/GrandPrize2009_BPC_BigChaos.pdf)
  * [https://kaggle2.blob.core.windows.net/wiki-files/327/09ccf652-8c1c-4a3d-b979-ce2369c985e4/Willem%20Mestrom%20-%20Milestone%201%20Description%20V2%202.pdf](https://kaggle2.blob.core.windows.net/wiki-files/327/09ccf652-8c1c-4a3d-b979-ce2369c985e4/Willem%20Mestrom%20-%20Milestone%201%20Description%20V2%202.pdf)
  
### 6.7.6 Model Based Prediction
- Assume the data follow a probabilistic model
- Use Bayes' theorem to identify optimal classifiers
Pros:
Can take advantage of structure of the data
May be computationally convenient
Are reasonably accurate on real problems
Cons:
Make additional assumptions about the data
When the model is incorrect you may get reduced accuracy
  
## 6.8 Model building
```{r}
modelFit <- train(type ~.,data=training, method="glm")
modelFit
```
Final model
```{r}
modelFit$finalModel
```
Prediction
```{r}
predictions <- predict(modelFit,newdata=testing)
predictions
```
Confusion Matrix
```{r}
confusionMatrix(predictions,testing$type)
```
### Resources on {caret}
- Caret tutorials:
  - [http://www.edii.uclm.es/~useR-2013/Tutorials/kuhn/user_caret_2up.pdf](http://www.edii.uclm.es/~useR-2013/Tutorials/kuhn/user_caret_2up.pdf)
  - [https://cran.r-project.org/web/packages/caret/vignettes/caret.html](https://cran.r-project.org/web/packages/caret/vignettes/caret.html)
- [A paper introducing the caret package](http://www.jstatsoft.org/v28/i05/paper)
- [Model training and tuning](http://caret.r-forge.r-project.org/training.html)
- [caret visualizations](http://topepo.github.io/caret/visualizations.html)

## Data resources
### Open Government Sites
* United Nations [http://data.un.org/](http://data.un.org/)
* U.S. [http://www.data.gov/](http://www.data.gov/)
  * [List of cities/states with open data](http://simplystatistics.org/2012/01/02/list-of-cities-states-with-open-data-help-me-find/)
* United Kingdom [http://data.gov.uk/](http://data.gov.uk/)
* France [http://www.data.gouv.fr/](http://www.data.gouv.fr/)
* Ghana [http://data.gov.gh/](http://data.gov.gh/)
* Australia [http://data.gov.au/](http://data.gov.au/)
* Germany [https://www.govdata.de/](https://www.govdata.de/) 
* Hong Kong [http://www.gov.hk/en/theme/psi/datasets/](http://www.gov.hk/en/theme/psi/datasets/)
* Japan [http://www.data.go.jp/](http://www.data.go.jp/)
* Many more [http://www.data.gov/opendatasites](http://www.data.gov/opendatasites)

### Gapminder
Gapminder has a lot of data about development in particular in human heath.
[http://www.gapminder.org/](http://www.gapminder.org/)
### Survey data from the United States
[http://www.asdfree.com/](http://www.asdfree.com/)
### Infochimps Marketplace
The Infochimps Marketplace has a bunch of different data sets which you can sort by various different tags.
[http://www.infochimps.com/marketplace](http://www.infochimps.com/marketplace)
### Kaggle
Kaggle is a company that offers data science competitions, and they often have very interesting data sets that they make available as part of those competitions. So they're good for practice, but they're also good for potentially discovering new, interesting things that can help companies solve real problems.
[http://www.kaggle.com/](http://www.kaggle.com/)
### Collections by data scientists
* Hilary Mason http://bitly.com/bundles/hmason/1
* Peter Skomoroch https://delicious.com/pskomoroch/dataset
* Jeff Hammerbacher http://www.quora.com/Jeff-Hammerbacher/Introduction-to-Data-Science-Data-Sets
* Gregory Piatetsky-Shapiro http://www.kdnuggets.com/gps.html
* [http://blog.mortardata.com/post/67652898761/6-dataset-lists-curated-by-data-scientists](http://blog.mortardata.com/post/67652898761/6-dataset-lists-curated-by-data-scientists)

### More specialized collections
* [Stanford Large Network Data](http://snap.stanford.edu/data/) focuses on network data, machine learning
* [UCI Machine Learning](http://archive.ics.uci.edu/ml/) has a variety of data sets that can be used to practice your classification or predictions
* [KDD Nugets Datasets](http://www.kdnuggets.com/datasets/index.html)
* [CMU Statlib](http://lib.stat.cmu.edu/datasets/) one of the most famous canonical sets of data sets
* [Gene expression omnibus](http://www.ncbi.nlm.nih.gov/geo/) focuses on data sets that come from human genomic experiments or other organismal genomics experiments
* [ArXiv Data](http://arxiv.org/help/bulk_data)
* [Public Data Sets on Amazon Web Services](http://aws.amazon.com/publicdatasets/)

### Some API's with R interfaces
* [twitter](https://dev.twitter.com/) and [twitteR](http://cran.r-project.org/web/packages/twitteR/index.html) package
* [figshare](http://api.figshare.com/docs/intro.html) and [rfigshare](http://cran.r-project.org/web/packages/rfigshare/index.html)
* [PLoS](http://api.plos.org/) and [rplos](http://cran.r-project.org/web/packages/rplos/rplos.pdf)
* [rOpenSci](http://ropensci.org/packages/index.html)
* [Facebook](https://developers.facebook.com/) and [RFacebook](http://cran.r-project.org/web/packages/Rfacebook/)
* [Google maps](https://developers.google.com/maps/) and [RGoogleMaps](http://cran.r-project.org/web/packages/RgoogleMaps/index.html)