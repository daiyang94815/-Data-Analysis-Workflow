---
title: "Data Analysis Workflow"
output:
---
# Step 0 Set working directory
```{r}
getwd()
setwd()
```
**Relative** - `setwd("./data")`, `setwd("../")` (Move up one directory)  
**Absolute** - `setwd("/Users/jtleek/data/")` "/" for path in windows
## Check for and create directories
```{r}
# check to see if the directory exists
file.exists("directoryName") 
# create a directory if it doesn't exist
dir.create("directoryName") 

# an example checking for a "data" directory and creating it if it doesn't exist
if(!file.exists("data")){
  dir.create("data")
}
```
# Packages
## {data.table}
Much, much faster at subsetting, group, and updating
### Create data tables just like data frames
```{r}
library(data.table)
DF = data.frame(x=rnorm(9),y=rep(c("a","b","c"),each=3),z=rnorm(9))
head(DF,3)
DT = data.table(x=rnorm(9),y=rep(c("a","b","c"),each=3),z=rnorm(9))
head(DT,3)
```
### See all the data tables in memory
```{r}
tables()
```
### Subset rows
```{r}
DT[2,]
DT[DT$y=="a",]
```
### Subsetting is based on rows (not columns like DF[c(2,3)])
```{r}
DT[c(2,3)]
```
**Data table use expression after comma**
### Calculate values for variables with expressions
```{r}
DT[,list(mean(x),sum(z))]
DT[,table(y)]
```
### Add new columns
```{r}
DT[,w:=z^2]
```
**When add a new variable to a data table, a new copy isn't being created.  If you're trying to create a copy, you have to explicitly do that with the copy function.** 
### Multiple operations
```{r}
DT[,m:= {tmp <- (x+z); log2(tmp+5)}]
```
### plyr like operations
```{r}
DT[,a:=x>0]
```

```{r}
DT[,b:= mean(x+w),by=a]
```
`by=a` aggregated/grouped by a. Then add aggregated values (mean in this case) of different groups to each row of b.
### Special variables
`.N` An integer, length 1, containing the number of elements (counts) of a factor level
```{r}
set.seed(123);
DT <- data.table(x=sample(letters[1:3], 1E5, TRUE))
DT[, .N, by=x]
```
### Keys
#### To subset
```{r}
DT <- data.table(x=rep(c("a","b","c"),each=100), y=rnorm(300))
setkey(DT, x)
DT['a']
```
#### To Join(merge)
```{r}
DT1 <- data.table(x=c('a', 'a', 'b', 'dt1'), y=1:4)
DT2 <- data.table(x=c('a', 'b', 'dt2'), z=5:7)
setkey(DT1, x); setkey(DT2, x)
merge(DT1, DT2)
```
### Fast reading from disk
```{r}
big_df <- data.frame(x=rnorm(1E6), y=rnorm(1E6))
file <- tempfile()
write.table(big_df, file=file, row.names=FALSE, col.names=TRUE, sep="\t", quote=FALSE)
system.time(fread(file))
system.time(read.table(file, header=TRUE, sep="\t"))
```
`fread()` could be applied to reading data tables, a substitute for `read.table()` with tab separated files
### Further resources
- [Latest development of data.table](https://github.com/Rdatatable/data.table/wiki)
- [A list of differences between data.table and data.frame](https://stackoverflow.com/questions/13618488/what-you-can-do-with-data-frame-that-you-cant-in-data-table)
## {sqldf}
sqldf package allows for execution of SQL commands on R data frames
# Step 1 Get the data
## From the internet
```{r}
# Download the file
fileUrl <- "https://data.baltimorecity.gov/api/views/dz54-2aru/rows.csv?accessType=DOWNLOAD"
download.file(fileUrl,destfile="./data/cameras.csv",mode = 'wb')

# Check the downloaded file
list.files("./data")
# Record the date downloaded
dateDownloaded <- date()
dateDownloaded
```
## From local flat files
### read.table
```{r}
cameraData <- read.table("./data/cameras.csv",sep=",",header=TRUE)
```
Important parameters
- `file`
- `header`
- `sep` - delimiter
- `row.names`
- `nrows` - how many rows to read of the file (e.g. nrows=10 reads 10 lines)
- `quote` - whether there are any quoted values, quote="" means no quotes
- `na.strings` - set the character that represents a missing value
- `skip` - number of lines to skip before starting to read
### read.csv
```{r}
cameraData <- read.csv("./data/cameras.csv")
```
`read.csv` sets `sep=","` and `header=TRUE`
## From Excel files
### read_excel {readxl}
```{r}
download.file(fileUrl,destfile="./data/cameras.xlsx",mode="wb")
library(readxl)
cameraData <- read_excel("./data/cameras.xlsx",sheet=1,col_names = TRUE,range=cell_limits(ul=c(18,7),lr=c(23,15)))
```
## From XML or Html
### {XML}
```{r}
library(XML)
fileUrl <- "http://www.w3schools.com/xml/simple.xml"
doc <- xmlTreeParse(fileUrl,useInternal=TRUE) # or htmlTreeParse() if from Html
```
`useInternal=TRUE` get all the different nodes inside of the file
#### Inspect the data 
```{r}
# get rootNode, the wrapper for the entire document
rootNode <- xmlRoot(doc)
# get the name of the document
xmlName(rootNode)
# get all the nested elements within the rootNode
names(rootNode)
```

```{r}
# Directly access parts of the XML document
rootNode[[1]]
# access subcomponent
rootNode[[1]][[1]]
```
#### Programatically extract parts of the file
```{r}
# get all values
xmlSApply(rootNode,xmlValue)
```
To be more specific, use [XPath](http://www.stat.berkeley.edu/~statcur/Workshop2/Presentations/XML.pdf)
- `/node` Top level node
- `//node` Node at any level
- `node[@attr-name]` Node with an attribute name
- `node[@attr-name='bob']` Node with attribute name attr-name='bob'
```{r}
# examples
xpathSApply(rootNode,"//name",xmlValue)
xpathSApply(rootNode,"//price",xmlValue)
```
##### Extract content by attributes
```{r}
# example
fileUrl <- "http://espn.go.com/nfl/team/_/name/bal/baltimore-ravens"
doc <- htmlTreeParse(fileUrl,useInternal=TRUE)
scores <- xpathSApply(doc,"//li[@class='score']",xmlValue)
teams <- xpathSApply(doc,"//li[@class='team-name']",xmlValue)
```
#### Notes and further resources
- Official XML tutorials [short](http://www.omegahat.org/RSXML/shortIntro.pdf), [long](http://www.omegahat.org/RSXML/shortIntro.pdf)
- [An outstanding guide to the XML package](https://www.stat.berkeley.edu/~statcur/Workshop2/Presentations/XML.pdf)
## From JSON
### {jsonlite}
```{r}
library(jsonlite)
jsonData <- fromJSON("https://api.github.com/users/jtleek/repos")
```
#### Inspect the data (nested objects)
```{r}
names(jsonData)
jsonData$name

names(jsonData$owner)
jsonData$owner$login
```
#### Writing data frames to JSON
```{r}
myjson <- toJSON(iris, pretty=TRUE)
# print the data
cat(myjson)
```
`pretty=TRUE` give nice indentations so it's easy to read
#### Further resources
- [http://www.json.org/](http://www.json.org/)
- [A good tutorial on jsonlite](https://www.r-bloggers.com/new-package-jsonlite-a-smarter-json-encoderdecoder/)
- [jsonlite vignette](https://cran.r-project.org/web/packages/jsonlite/vignettes/json-mapping.pdf)
## From MySQL
### {RMySQL}
#### Connect and list databases
```{r}
library(RMySQL)
ucscDb <- dbConnect(MySQL(),user="genome", 
                    host="genome-mysql.cse.ucsc.edu")
result <- dbGetQuery(ucscDb,"show databases;"); dbDisconnect(ucscDb)
```
#### Connect to specific database and list tables
```{r}
hg19 <- dbConnect(MySQL(),user="genome", db="hg19",
                    host="genome-mysql.cse.ucsc.edu")
allTables <- dbListTables(hg19)
length(allTables)
allTables[1:5]
```
#### Get dimensions of a specific table
```{r}
# get column names
dbListFields(hg19,"affyU133Plus2")
# get number of rows
dbGetQuery(hg19, "select count(*) from affyU133Plus2")
```
#### Read from the table
```{r}
affyData <- dbReadTable(hg19, "affyU133Plus2")
head(affyData)
```
##### Select a specific subset
```{r}
query <- dbSendQuery(hg19, "select * from affyU133Plus2 where misMatches between 1 and 3")
affyMis <- fetch(query)
affyMisSmall <- fetch(query,n=10); dbClearResult(query)
dim(affyMisSmall)
```
#### Close the connection
```{r}
dbDisconnect(hg19)
```
#### Further resources
- [RMySQL vignette](https://cran.r-project.org/web/packages/RMySQL/RMySQL.pdf)
- [List of MySQL commands](http://www.pantz.org/software/mysql/mysqlcommands.html)
- A nice blog post summarizing [some other commands](https://www.r-bloggers.com/mysql-and-r/)
## From HDF5
### {rhdf5}
```{r}
source("http://bioconductor.org/biocLite.R")
biocLite("rhdf5")
library(rhdf5)
```
#### Create HDF5 file
```{r}
created = h5createFile("example.h5")
```
##### Create groups
```{r}
created = h5createGroup("example.h5","foo")
created = h5createGroup("example.h5","baa")
created = h5createGroup("example.h5","foo/foobaa")
h5ls("example.h5")
```
##### Write to groups
```{r}
A = matrix(1:10,nr=5,nc=2)
h5write(A, "example.h5","foo/A")
B = array(seq(0.1,2.0,by=0.1),dim=c(5,2,2))
attr(B, "scale") <- "liter"
h5write(B, "example.h5","foo/foobaa/B")
h5ls("example.h5")
```
##### Write a data set
```{r}
df = data.frame(1L:5L,seq(0,1,length.out=5),
  c("ab","cde","fghi","a","s"), stringsAsFactors=FALSE)
h5write(df, "example.h5","df")
h5ls("example.h5")
```
#### Read data
```{r}
readA = h5read("example.h5","foo/A")
readB = h5read("example.h5","foo/foobaa/B")
readdf= h5read("example.h5","df")
readA
```
#### Write and read chunks(subsets)
```{r}
h5write(c(12,13,14),"example.h5","foo/A",index=list(1:3,1))
h5read("example.h5","foo/A")
```
`index=list(1:3,1)` give you indices for the dimensions to read/write
#### Further resources
- [The rhdf5 tutorial](http://www.bioconductor.org/packages/release/bioc/html/rhdf5.html)
- The HDF group has [informaton on HDF5 in general](https://portal.hdfgroup.org/display/HDF5/HDF5)
## From Web
### readLines()
```{r}
# open a connection
con = url("http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en")
htmlCode = readLines(con)
# close the connection
close(con)
```
### {XML}
```{r}
library(XML)
url <- "http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en"
html <- htmlTreeParse(url, useInternalNodes=T)

xpathSApply(html, "//title", xmlValue)
xpathSApply(html, "//td[@id='col-citedby']", xmlValue)
```
`useInternalNodes=T` gets the complete structure out
### GET() from {httr}
```{r}
library(httr)
# GET the URL
html2 = GET(url)
# extract the content from that HTML page as a text
content2 = content(html2,as="text")
# parse the content
parsedHtml = htmlParse(content2,asText=TRUE)
xpathSApply(parsedHtml, "//title", xmlValue)
```
#### Access websites with passwords
```{r}
pg2 = GET("http://httpbin.org/basic-auth/user/passwd",
    authenticate("user","passwd"))
```
##### Use handles (to store authentication)
```{r}
google = handle("http://google.com")
pg1 = GET(handle=google,path="/")
pg2 = GET(handle=google,path="search")
```
#### Further resources
- [R Bloggers](https://www.r-bloggers.com/) has a number of examples of web scraping
- The [httr help file](https://cran.r-project.org/web/packages/httr/httr.pdf) has useful examples
## From APIs
### {httr}
#### Access Website from R
```{r}
# example 1: Twitter
library(httr)
myapp = oauth_app("twitter",
                   key="yourConsumerKeyHere",secret="yourConsumerSecretHere")
sig = sign_oauth1.0(myapp,
                     token = "yourTokenHere",
                      token_secret = "yourTokenSecretHere")
homeTL = GET("https://api.twitter.com/1.1/statuses/home_timeline.json", sig)
```

```{r}
# example 2: GitHub
# 1. Find OAuth settings for github:
#    http://developer.github.com/v3/oauth/
oauth_endpoints("github")
# 2. To make your own application, register at 
#    https://github.com/settings/developers. Use any URL for the homepage URL
#    (http://github.com is fine) and  http://localhost:1410 as the callback url
#    Replace your key and secret below.
myapp <- oauth_app("datasciencespecialization",
                   key = "key",
                   secret = "secret")
# 3. Get OAuth credentials
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
# 4. Use API
gtoken <- config(token = github_token)
req <- GET("https://api.github.com/users/jtleek/repos", gtoken)
stop_for_status(req)
content(req)
# OR:
req <- with_config(gtoken, GET("https://api.github.com/users/jtleek/repos"))
stop_for_status(req)
content(req)
```
#### Convert the json object
```{r}
# example 1
# return a structured R object, which is a little bit hard to read
json1 = content(homeTL)
```
### {jsonlite}
```{r}
# example 1
# reformat it as a data frame where each row corresponds to a tweet in the user's timeline
json2 = jsonlite::fromJSON(toJSON(json1))
```

```{r}
# example 2
library(jsonlite)
repo <- fromJSON(toJSON(content(req)))
```
## From image
- [jpeg](https://cran.r-project.org/web/packages/jpeg/index.html)
- [readbitmap](https://cran.r-project.org/web/packages/readbitmap/index.html)
- [png](https://cran.r-project.org/web/packages/png/index.html)
- [EBImage (Bioconductor)](http://www.bioconductor.org/packages/2.13/bioc/html/EBImage.html)
## From GIS data
- [rgdal](https://cran.r-project.org/web/packages/rgdal/index.html)
- [rgeos](https://cran.r-project.org/web/packages/rgeos/index.html)
- [raster](https://cran.r-project.org/web/packages/raster/index.html)
## From music data
- [tuneR](https://cran.r-project.org/web/packages/tuneR/)
- [seewave](http://rug.mnhn.fr/seewave/)
# Step 2 Clean the data
## Subset
```{r}
# subset by colunmns, x is a data frame
X[,1]
# or
X[1]
# or
X[,'var1']

# subset by rows and colunmns
X[1:2,'var2']
```
### select(), filter() from {dplyr} or {tidyverse}
```{r}
donor %>% filter(amount > 100)
donor %>% filter(! type %in% 'Candidate')
```
Operators for conditions: `!, %in%, >, >=, <, <=, ==, &, |`
```{r}
select(chicago, 1:5)
select(chicago, city:dptp)
select(chicago, -(city:dptp))
```
### Logicals (and, or, %in%)
```{r}
X[(X$var1 <= 3 & X$var3 > 11),]
X[(X$var1 <= 3 | X$var3 > 15),]
restData[restData$zipCode %in% c("21212","21213"),]
```
### Deal with missing values
```{r}
X[which(X$var2 > 8),]
```
`which()` returns the indices where conditions are met
## Reorder
### arrange() from {dplyr} or {tidyverse}
```{r}
arrange(chicago, desc(date))
```
### sort()
```{r}
sort(X$var1)
sort(X$var1,decreasing=TRUE)
sort(X$var2,na.last=TRUE)
```
### order()
```{r}
X[order(X$var1),]
X[order(X$var1,X$var3),]
```
`order()` returns a permutation which rearranges its first argument into ascending or descending order, breaking ties by further arguments.
### {plyr}
```{r}
library(plyr)
arrange(X,var1)
arrange(X,desc(var1))
```
## Add rows and columns
```{r}
X$var4 <- rnorm(5)
```
### Binding from {dplyr} or {tidyverse}
Append z to y as new rows. 
```{r}
bind_rows(y, z) 
```
Append z to y as new columns.
```{r}
bind_cols(y, z) 
```
Caution: matches rows by position.
### cbind() and rbind()
```{r}
Y <- cbind(X,rnorm(5))
```
## Rename() from {dplyr} or {tidyverse}
```{r}
rename(chicago, dewpoint = dptp, pm25 = pm25tmean2)
```
## Create new variables
### mutate() from {dplyr} or {tidyverse}
```{r}
library(tidyverse)
restData2 = mutate(restData,zipGroups=cut2(zipCode,g=4))
```
### Create sequences
```{r}
s1 <- seq(1,10,by=2)
s2 <- seq(1,10,length=3)
x <- c(1,3,8,25,100); seq(along = x)
```
### Create binary variables
```{r}
restData$nearMe = restData$neighborhood %in% c("Roland Park", "Homeland")
restData$zipWrong = ifelse(restData$zipCode < 0, TRUE, FALSE)
```
### Create categorical variables
```{r}
restData$zipGroups = cut(restData$zipCode,breaks=quantile(restData$zipCode))
```
apply `cut()` to quantitative variable (zipCode). `breaks=` break up according to a list of value (the quantiles to that zip code). Returns a factor variable.
#### cut2() from {Hmisc}
```{r}
library(Hmisc)
restData$zipGroups = cut2(restData$zipCode,g=4)
```
`g=` number of quantile groups
### Create factor variables
```{r}
restData$zcf <- factor(restData$zipCode)
```
#### Levels of factor variables
```{r}
yesnofac = factor(yesno,levels=c("yes","no"))
relevel(yesnofac,ref="no")
```
if no `levels=c("yes","no")`, by default, `factor()` treats the lowest value alphabetically as the first factor variable.
```{r}
schoolPub$High.Grade.=factor(schoolPub$High.Grade.,
                             levels = levelCat,
                             labels = levelCat,
                             ordered = T)
```
### Common transforms
- `abs(x)` absolute value
- `sqrt(x)` square root
- `ceiling(x)` ceiling(3.475) is 4
- `floor(x)` floor(3.475) is 3
- `round(x,digits=n)` round(3.475,digits=2) is 3.48
- `signif(x,digits=n)` signif(3.475,digits=2) is 3.5
- `cos(x)`, `sin(x)` etc.
- `log(x)` natural logarithm
- `log2(x)`, `log10(x)` other common logs
- `exp(x)` exponentiating x
- [http://statmethods.net/management/functions.html](http://statmethods.net/management/functions.html)
### Further resources
- A [tutorial](http://plyr.had.co.nz/09-user/) from the developer of plyr
- Andrew Jaffe's [R notes](http://www.biostat.jhsph.edu/~ajaffe/lec_winterR/Lecture%202.pdf)
- A nice [lecture on categorical and factor variables](https://www.stat.berkeley.edu/classes/s133/factors.html)
## Reshape the data
### gather(), spread() from {tidyr} or {tidyverse}
#### Convert columns into rows (with `gather()`)
```{r}
donor %>% gather(column_name, year, c(receipt_year, election_year))
```
`key=column_name` (new) name of column that store (old) column names  
`value=year` (new) name of column that store (old) values in (old) columns  
`c(receipt_year, election_year)` A selection of columns. If empty, all variables are selected. You can supply bare variable names, select all variables between x and z with `x:z`, exclude y with `-y`.
#### Convert rows into columns (with `spread()`)
```{r}
donor %>% spread(receipt_year, amount)
```
`key=receipt_year` (old) name or position of column that contains the names of the (new) columns  
`value=amount` (old) name of column that contains the values of the (new) columns
### {reshape2}
```{r}
library(reshape2)
```
#### melt() data frames
```{r}
mtcars$carname <- rownames(mtcars)
carMelt <- melt(mtcars,id=c("carname","gear","cyl"),measure.vars=c("mpg","hp"))
```
`id=` creates an id value for car name, for gear and for cylinders. And then melt all the rest of the values. 
#### Cast (dcast()) data frames
```{r}
cylData <- dcast(carMelt, cyl ~ variable)
cylData <- dcast(carMelt, cyl ~ variable,mean)
```
Default aggregation function is length()
### Aggregate values/ Create a new variable
#### group_by(), summarise() from {dplyr} or {tidyverse}
##### Example 1
In `donor`, which `type` received the more money in donations from `party` 'REPUBLICAN'?
```{r}
donor %>% group_by(type, party) %>% summarise(dollars = sum(amount, na.rm = TRUE))
```
- `group_by()` lists the variables by which to aggregate data
- `summarise()` creates a variable (dollars) and define the variable with an aggregation function
- Use `%>%` to 'link' `group_by()` and `summarise()`
- Aggregation functions: `n(), n_distinct(), sum(), mean(), max(), min()`, etc.
- `options(pillar.sigfig = 10)` to see more digits for numeric results in tibble
##### Example 2
In `donor`, what is the largest average donation `amount` for `contributor_state`?
```{r}
donor %>% 
  group_by(contributor_state) %>% 
  summarise(avg_donation = mean(amount, na.rm = TRUE)) %>%
  arrange(desc(avg_donation))
```
##### Example 3
```{r}
police %>% 
  transmute(
    initial_type_group = initial_type_group %>% as.character()
    , district_sector = district_sector %>% as.character()
    , event_clearance_group = event_clearance_group %>% as.character()
  ) %>%
  mutate(
    initial_type_group = ifelse(initial_type_group %in% NA, 'Unknown', initial_type_group)
  ) %>%
  filter(event_clearance_group %in% 'DISTURBANCES') %>% 
  group_by(initial_type_group, district_sector) %>% 
  summarise(n  = n()) %>%
  write_csv('tidyverse_exercise_output.csv')
```
##### Example 4
```{r}
police %>% 
  transmute(
    initial_type_group = initial_type_group %>% as.character()
    , district_sector = district_sector %>% as.character()
    , event_clearance_group = event_clearance_group %>% as.character()
  ) %>%
  mutate(
    initial_type_group = ifelse(initial_type_group %in% NA, 'Unknown', initial_type_group)
  ) %>%
  filter(event_clearance_group %in% 'DISTURBANCES') %>% 
  group_by(initial_type_group, district_sector) %>% 
  summarise(n  = n()) %>%
  filter(
    (initial_type_group %in% 'Unknown' & district_sector %in% 'W') |
    (initial_type_group %in% 'ROAD RAGE' & district_sector %in% 'L')
  )
```
#### tapply()
```{r}
tapply(InsectSprays$count,InsectSprays$spray,sum)
```
sum up the counts, break down by spray
#### Another way - split()+apply()+unlist()
```{r}
spIns =  split(InsectSprays$count,InsectSprays$spray)
```
`split()` returns a list
```{r}
sprCount = lapply(spIns,sum)
unlist(sprCount)
```
#### Another way - split()+sapply()
```{r}
sapply(spIns,sum)
```
#### ddply() from {plyr}
```{r}
library(plyr)
ddply(InsectSprays,.(spray),summarize,sum=sum(count))
```
`.()` are the variables to summarize
```{r}
spraySums <- ddply(InsectSprays,.(spray),summarize,sum=ave(count,FUN=sum))
```
### Further resources
- A [tutorial](http://plyr.had.co.nz/09-user/) from the developer of plyr
- [A nice reshape tutorial](https://www.slideshare.net/jeffreybreen/reshaping-data-in-r)
- [A good plyr primer](https://www.r-bloggers.com/a-quick-primer-on-split-apply-combine-problems/)
## Merge the data
### joins from {dplyr} or {tidyverse}
#### Mutating Joins
Join matching rows from b to a.
```{r}
left_join(a, b, by = "x1")
```
Join matching rows from a to b. 
```{r}
right_join(a, b, by = "x1") 
```
Join data. Retain only rows in both sets. 
```{r}
inner_join(a, b, by = "x1") 
```
Join data. Retain all values, all rows.
```{r}
full_join(a, b, by = "x1") 
```
#### Filtering Joins
All rows in a that have a match in b. 
```{r}
semi_join(a, b, by = "x1") 
```
All rows in a that do not have a match in b
```{r}
anti_join(a, b, by = "x1") 
```
#### Set Operations
Rows that appear in both y and z. 
```{r}
intersect(y, z) 
```
Rows that appear in either or both y and z.
```{r}
union(y, z)
```
Rows that appear in y but not z.
```{r}
setdiff(y, z)
```
### merge()
```{r}
mergedData = merge(reviews,solutions,by.x="solution_id",by.y="id",all=TRUE)
```
`all=TRUE` full join
### Further resourece
- [The quick R data merging page](https://www.statmethods.net/management/merging.html)
## Summarize the data
### Look at a bit of the data
```{r}
head(restData,n=3)
tail(restData,n=3)
```
### summary()
```{r}
summary(restData)
```
### More in depth information
```{r}
str(restData)
```
### Distribution
#### Quantiles of quantitative variables
```{r}
quantile(restData$councilDistrict,na.rm=TRUE)
quantile(restData$councilDistrict,probs=c(0.5,0.75,0.9))
```
#### table()
```{r}
table(restData$zipCode,useNA="ifany")
table(restData$councilDistrict,restData$zipCode)
```
`useNA="ifany"` if there are any missing values, they'll be an added column to this table, which will be NA, and it'll tell you the number of missing values there is.
### Check for missing values
```{r}
sum(is.na(restData$councilDistrict))
any(is.na(restData$councilDistrict))
all(restData$zipCode > 0)
```
#### Row and column sums
```{r}
colSums(is.na(restData))
all(colSums(is.na(restData))==0)
```
### Values with specific characteristics
```{r}
table(restData$zipCode %in% c("21212","21213"))
```
### Cross tabs
```{r}
xt <- xtabs(Freq ~ Gender + Admit,data=DF)
```
`Freq` the variable that you want to be displayed in the table  
`Gender + Admit` break that down by variables
```{r}
xt = xtabs(breaks ~.,data=warpbreaks)
```
`~.` break down by all the variables in the data set
### Flat tables
```{r}
ftable(xt)
```
`ftable` make flat tables from the crosstabs. It will summarize the data in a much smaller, more compact form. So it's easier to see.
### Size of a data set
```{r}
object.size(fakeData)
print(object.size(fakeData),units="Mb")
```
## Edit text variables
### transformation of upper/lower case
```{r}
tolower(names(cameraData))
toupper()
```
### split
#### strsplit()+sapply()
```{r}
splitNames = strsplit(names(cameraData),"\\.")
```
`split="\\."` character vector containing regular expression(s) to use for splitting.
- strsplit() returns a list
- use escape character '\' because the period is a reserved character
Then extract the first element of splited names:
```{r}
splitNames[[6]][1]
firstElement <- function(x){x[1]}
sapply(splitNames,firstElement)
```
### substitute
#### sub() and gsub()
```{r}
testName <- "this_is_a_test"

# sub only the first one
sub("_","",testName)

# sub all
gsub("_","",testName)
```
### search and find values
#### grep() and grepl()
```{r}
grep("Alameda",cameraData$intersection)
table(grepl("Alameda",cameraData$intersection))
grep("Alameda",cameraData$intersection,value=TRUE)

# subset
cameraData2 <- cameraData[!grepl("Alameda",cameraData$intersection),]
```
`pattern="Alameda"` character string containing a regular expression (or character string for fixed = TRUE) to be matched
- `grep(value = FALSE)` returns a vector of the indices of the elements of x that yielded a match (or not, for `invert = TRUE`)
- `grep(value = TRUE)` returns a character vector containing the selected elements of x
- `grepl` returns a logical vector (match or not for each element of x)
### more string functions
#### {stringr}
```{r}
# number of characters
nchar("Jeffrey Leek")
# extract parts of the string
substr("Jeffrey Leek",1,7) # 1st to 7th characters
# paste to get a string spereated with a space
paste("Jeffrey","Leek")
# paste to get a string without space
paste0("Jeffrey","Leek")
# removes whitespace from start and end of string
str_trim("Jeff      ")
```
## Regular expression
### metacharacter
`^` represents the start of a line
```{r}
^i think
```
`$` represents the end of a line
```{r}
morning$
```
`.` is used to refer to any character
```{r}
9.11
```
`|` "or"
```{r}
flood|fire
flood|earthquake|hurricane|coldfire
```
`?` indicates that the indicated expression is optional
```{r}
[Gg]eorge( [Ww]\.)? [Bb]ush
```
we wanted to match a "." as a literal period; to do that, we had to "escape" the metacharacter, preceding it with a backslash In general, we have to do this for any metacharacter we want to include in our match  
`*` and `+` indicate repetition; `*` means "any number, including none, of the item" and `+` means "at least one of the item"
```{r}
(.*)
[0-9]+ (.*)[0-9]+
```
`{}` are referred to as interval quantifiers which specify the minimum and maximum number of matches of an expression
```{r}
[Bb]ush ([^ ]+ +){1,5}debate
```
- `m,n` m<=matches<=n
- `m` matches==m
- `m,` matches>=m
`()` not only limits the scope of alternatives divided by a `|`, but also can be used to "remember" text matched by the subexpression enclosed; We refer to the matched text with `\1`, `\2`, etc.
```{r}
+([a-zA-Z]+) +\1 +
```
The greediness of `*` (longest possible match) can be turned off with the `?`
```{r}
^s(.*?)s$
```
### Character Classes with []
list a set of characters we will accept at a given point in the match
```{r}
[Bb][Uu][Ss][Hh]

^[Ii] am

^[0-9][a-zA-Z]

^[Gg]ood|[Bb]ad
^([Gg]ood|[Bb]ad)
```
When used at the beginning of a character class, the "^" is also a metacharacter and indicates matching characters NOT in the indicated class
```{r}
[^?.]$
```
## Work with date
### format date
- `%d` day as number (0-31)
- `%a` abbreviated weekday
- `%A` unabbreviated weekday
- `%m` month (00-12)
- `%b` abbreviated month
- `%B` unabbrevidated month
- `%y` 2 digit year
- `%Y` four digit year
```{r}
format(Sys.Date(),"%a %b %d")
```
### create date
```{r}
x = c("1jan1960", "2jan1960", "31mar1960", "30jul1960")
z = as.Date(x, "%d%b%Y")
z[1] - z[2]
as.numeric(z[1]-z[2])
```
### attribute of date
```{r}
weekdays(d2)
months(d2)

# days from origin
julian(d2) 
```
### {lubridate}
```{r}
library(lubridate)
ymd("20140108")
mdy("08/04/2013")
dmy("03-04-2013")

ymd_hms("2011-08-03 10:15:03")
ymd_hms("2011-08-03 10:15:03",tz="Pacific/Auckland")
?Sys.timezone

x = dmy(c("1jan2013", "2jan2013", "31mar2013", "30jul2013"))
# weekday
wday(x[1])
# show weekday abbreviation
wday(x[1],label=TRUE)
```
#### Further resources
- More information in this nice [lubridate tutorial](http://www.r-statistics.com/2012/03/do-more-with-dates-and-times-in-r-with-lubridate-1-1-0/)
- The [lubridate vignette](http://cran.r-project.org/web/packages/lubridate/vignettes/lubridate.html) is the same content
- Ultimately you want your dates and times as class "Date" or the classes "POSIXct", "POSIXlt". For more information type `?POSIXlt`
## Data resources
### Open Government Sites
* United Nations [http://data.un.org/](http://data.un.org/)
* U.S. [http://www.data.gov/](http://www.data.gov/)
  * [List of cities/states with open data](http://simplystatistics.org/2012/01/02/list-of-cities-states-with-open-data-help-me-find/)
* United Kingdom [http://data.gov.uk/](http://data.gov.uk/)
* France [http://www.data.gouv.fr/](http://www.data.gouv.fr/)
* Ghana [http://data.gov.gh/](http://data.gov.gh/)
* Australia [http://data.gov.au/](http://data.gov.au/)
* Germany [https://www.govdata.de/](https://www.govdata.de/) 
* Hong Kong [http://www.gov.hk/en/theme/psi/datasets/](http://www.gov.hk/en/theme/psi/datasets/)
* Japan [http://www.data.go.jp/](http://www.data.go.jp/)
* Many more [http://www.data.gov/opendatasites](http://www.data.gov/opendatasites)

### Gapminder
Gapminder has a lot of data about development in particular in human heath.
[http://www.gapminder.org/](http://www.gapminder.org/)
### Survey data from the United States
[http://www.asdfree.com/](http://www.asdfree.com/)
### Infochimps Marketplace
The Infochimps Marketplace has a bunch of different data sets which you can sort by various different tags.
[http://www.infochimps.com/marketplace](http://www.infochimps.com/marketplace)
### Kaggle
Kaggle is a company that offers data science competitions, and they often have very interesting data sets that they make available as part of those competitions. So they're good for practice, but they're also good for potentially discovering new, interesting things that can help companies solve real problems.
[http://www.kaggle.com/](http://www.kaggle.com/)
### Collections by data scientists
* Hilary Mason http://bitly.com/bundles/hmason/1
* Peter Skomoroch https://delicious.com/pskomoroch/dataset
* Jeff Hammerbacher http://www.quora.com/Jeff-Hammerbacher/Introduction-to-Data-Science-Data-Sets
* Gregory Piatetsky-Shapiro http://www.kdnuggets.com/gps.html
* [http://blog.mortardata.com/post/67652898761/6-dataset-lists-curated-by-data-scientists](http://blog.mortardata.com/post/67652898761/6-dataset-lists-curated-by-data-scientists)

### More specialized collections
* [Stanford Large Network Data](http://snap.stanford.edu/data/) focuses on network data, machine learning
* [UCI Machine Learning](http://archive.ics.uci.edu/ml/) has a variety of data sets that can be used to practice your classification or predictions
* [KDD Nugets Datasets](http://www.kdnuggets.com/datasets/index.html)
* [CMU Statlib](http://lib.stat.cmu.edu/datasets/) one of the most famous canonical sets of data sets
* [Gene expression omnibus](http://www.ncbi.nlm.nih.gov/geo/) focuses on data sets that come from human genomic experiments or other organismal genomics experiments
* [ArXiv Data](http://arxiv.org/help/bulk_data)
* [Public Data Sets on Amazon Web Services](http://aws.amazon.com/publicdatasets/)

### Some API's with R interfaces
* [twitter](https://dev.twitter.com/) and [twitteR](http://cran.r-project.org/web/packages/twitteR/index.html) package
* [figshare](http://api.figshare.com/docs/intro.html) and [rfigshare](http://cran.r-project.org/web/packages/rfigshare/index.html)
* [PLoS](http://api.plos.org/) and [rplos](http://cran.r-project.org/web/packages/rplos/rplos.pdf)
* [rOpenSci](http://ropensci.org/packages/index.html)
* [Facebook](https://developers.facebook.com/) and [RFacebook](http://cran.r-project.org/web/packages/Rfacebook/)
* [Google maps](https://developers.google.com/maps/) and [RGoogleMaps](http://cran.r-project.org/web/packages/RgoogleMaps/index.html)

## Further resources
- [Data Wrangling with {dplyr} and {tidyr} Cheat Sheet](https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf)
- Andrew Jaffe's [lecture notes](http://www.biostat.jhsph.edu/~ajaffe/lec_winterR/Lecture%202.pdf)